{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import poisson\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture # For Expectation maximisation algorithm\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = np.genfromtxt('scale-d-10d.csv', delimiter=' ')\n",
    "data = data[:300,:10]\n",
    "\n",
    "labels = np.genfromtxt('scale-d-10d.csv', delimiter=' ',dtype=\"|U5\")\n",
    "labels = labels[:300,10]\n",
    "\n",
    "for i in range (labels.shape[0]):\n",
    "    labels[i] = float(labels[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array ([[0.15, 0.30, 0.00, 0.80], #c1\n",
    "                       [0.12, 0.29, 0.35, 0.69], #c1\n",
    "                       [0.00, 0.45, 0.61, 0.27], #c1\n",
    "                       [0.39, 0.00, 1.00, 0.10], #c2\n",
    "                       [0.59, 0.80, 0.10, 1.00], #c2\n",
    "                       [0.54, 0.90, 0.29, 0.63], #c2\n",
    "                       \n",
    "                       [0.69, 1.00, 0.59, 0.28],\n",
    "                       [1.00, 0.60, 0.81, 0.00],\n",
    "                       [0.20, 0.27, 0.39, 0.97], #c1\n",
    "                       [0.03, 0.47, 0.08, 0.57]])#c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = np.genfromtxt ('housing.csv')\n",
    "#house_data = house_data[:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data, syn_label = make_blobs(n_samples=100,\n",
    "                                 n_features=3,\n",
    "                                 centers=3,\n",
    "                                 cluster_std=0.5,\n",
    "                                 random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (syn_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data = np.vstack ((syn_data, [1.0, 1.0, 1.0]))\n",
    "print (syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "sub = fig.add_subplot(111, projection='3d')\n",
    "sub.scatter(syn_data[:,0], syn_data[:,1], syn_data[:,2]) #, c=col)\n",
    "sub.set_title(\"Synthetic Data\")\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter (X[:,0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P3C:\n",
    " # General class functionality (Robert)\n",
    "    \n",
    "    #class variables\n",
    "    _alpha = 0.001 #alpha for chi-squared-test\n",
    "    _EM_iter = 10 #iterations for EM\n",
    "\n",
    "    #Poisson threshold is the only parameter for P3C \n",
    "    def __init__(self, poisson_threshold): \n",
    "        \n",
    "        #sklearn-like attributes\n",
    "        self.labels_ = [] \n",
    "        self.cluster_centers_ = None #\"cluster cores\"\n",
    "        \n",
    "        #internally used variables\n",
    "        self._X = None\n",
    "        self._poisson_threshold = poisson_threshold\n",
    "        self._support_set = [] #for the bins\n",
    "        self._supports = [] #for the bins\n",
    "        self._approx_proj = [] #approximate projections\n",
    "        self._support_set_of_cores = []\n",
    "        self._fuzzy_membership_matrix = None\n",
    "    \n",
    "    # Methods for 3.1: Projections of true p-signatures (Mahdi and Robert)\n",
    "\n",
    "    def __normalize(self, X):\n",
    "        '''Normalizes the data featurewise\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array of the input data  \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Normalized data\n",
    "        '''\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "    \n",
    "    def __uniformity_test(self, attr):\n",
    "        '''Uses the chi-squared test to determine if the attribute is uniformly disributed,\n",
    "        using the class variable self._alpha as a certrainty-threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        attr: input list to be tested for uniformity \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True, if input data is uniform\n",
    "        '''\n",
    "        \n",
    "        if not np.any(attr):\n",
    "            return True\n",
    "        if chisquare(attr)[0] > self._alpha:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def __compute_support(self):\n",
    "        '''Computes Supports of each bin'''\n",
    "        \n",
    "        n = self._X.shape[0] # n = number of data objects\n",
    "        attribute_number = self._X.shape[1] # number of attributes \n",
    "        bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "\n",
    "        for i in range(attribute_number):\n",
    "            # support set of each interval S for attribiute i\n",
    "            supp_set = [[]for i in range(bin_number)]\n",
    "\n",
    "            interval_length = 1 / bin_number\n",
    "\n",
    "            # calculate in which bin should the point be placed based on attribute i\n",
    "            for j in range(n):\n",
    "                supp_set_index = math.floor(self._X[j,i]/interval_length)\n",
    "\n",
    "                if supp_set_index == len(supp_set):\n",
    "                    supp_set_index-=1 \n",
    "\n",
    "                supp_set[supp_set_index].append(self._X[j,:])\n",
    "\n",
    "            self._support_set.append(supp_set)  \n",
    "\n",
    "        self._supports = [[] for i in range(len(self._support_set))]\n",
    "        for i in range(len(self._support_set)):\n",
    "            for j in range(len(self._support_set[i])):\n",
    "                self._supports[i].append(len(self._support_set[i][j]))\n",
    "    \n",
    "    def __approximate_projection(self):\n",
    "        \n",
    "        '''Finds the bins that violate uniform distribution from the data stored in self._supports and\n",
    "        storest them in the numpy.array bins, with 1 for non-uniformity and 0 for uniformity. It then \n",
    "        assignes the intervals to self._approx_proj'''\n",
    "        \n",
    "        bins = np.zeros((len(self._supports), len(self._supports[0])), dtype=int)\n",
    "        for attr_number in range(len(self._supports)): #loop over all attributes\n",
    "                            \n",
    "            supp = self._supports[attr_number].copy() #make a copy of the supports of the current attribute\n",
    "            while self.__uniformity_test(supp) == False: #if not uniform find highest element\n",
    "                max_index = supp.index(max(supp))\n",
    "                supp.pop(max_index) #remove highest element from list\n",
    "                \n",
    "                i = 0 \n",
    "                while i <= max_index: #loop to adjust max_index according to previousely deleted elements\n",
    "                    max_index += bins[attr_number, i]\n",
    "                    i += 1\n",
    "                bins[attr_number, max_index]  = 1 #mark highest bin\n",
    "            \n",
    "            interval_list = [] #2d list for current attribute\n",
    "            interval = [] #current interval\n",
    "            open_interval = False\n",
    "\n",
    "            for i in range(len(bins[attr_number])):\n",
    "                if open_interval == False: #open new interval\n",
    "                    if bins[attr_number, i] == 1:\n",
    "                        interval.append(i/len(bins[attr_number]))\n",
    "                        open_interval = True\n",
    "                if open_interval == True: #close current interval\n",
    "                    if bins[attr_number, i] == 0:\n",
    "                        interval.append(i/len(bins[attr_number]))\n",
    "                        interval_list.append (interval)\n",
    "                        interval = []\n",
    "                        open_interval = False\n",
    "                    if (i == len(bins[attr_number])-1) and (bins[attr_number, i] == 1): #last bin marked 1\n",
    "                        interval.append ((i+1)/len(bins[attr_number]))\n",
    "                        interval_list.append (interval)                \n",
    "\n",
    "            self._approx_proj.append (interval_list)\n",
    "    \n",
    "    \n",
    "    # Methods for 3.2: Cluster Cores (Akshey and Jonas)\n",
    "    \n",
    "    def __convert_approx_proj_to_dict(self):\n",
    "        ''' Converts _approx_proj to a dictonary\n",
    "        This is necessary to compute cluster cores with apriori'''\n",
    "        \n",
    "        _approx_proj_sig = []\n",
    "        for attribute, row in enumerate(self._approx_proj):\n",
    "            for interval in row:\n",
    "                _approx_proj_sig.append({attribute:interval})\n",
    "                \n",
    "        return _approx_proj_sig\n",
    "    \n",
    "    def __compute_support_sig(self, p_signature):\n",
    "        '''Computes support for p-signature\n",
    "        This function computes the support by removing data points\n",
    "        that do not lie in any of the intervals of the given p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data.shape[0] : number of points in p-signature\n",
    "        '''\n",
    "\n",
    "        data = np.copy(self._X)\n",
    "        for attribute in p_signature:\n",
    "            interval = p_signature[attribute]\n",
    "            remove = []\n",
    "            for i, point in enumerate(data):\n",
    "                if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                    remove.append(i)\n",
    "            data = np.delete(data, remove, 0)\n",
    "\n",
    "        return data.shape[0]\n",
    "    \n",
    "    def __compute_exp_support(self, p_signature, interval):\n",
    "        ''' Computes expected support for a p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        interval : list with start and end value of interval\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        support * width\n",
    "        '''\n",
    "\n",
    "        support = self.__compute_support_sig(p_signature)\n",
    "        width = abs(interval[0] - interval[1])\n",
    "\n",
    "        return support*width\n",
    "    \n",
    "    def __diff_interval(self, p_signature, pplus1_signature):\n",
    "        '''Helper function to compute difference in interval for two p-signatures.\n",
    "           Used for possion threshold\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interval\n",
    "        '''\n",
    "\n",
    "        diff = list(set(pplus1_signature) - set(p_signature))\n",
    "        interval = pplus1_signature[diff[0]]\n",
    "        \n",
    "        return interval\n",
    "\n",
    "    def __check_core_condition(self, p_signature, pplus1_signature):\n",
    "        ''' Checks if probability is smaller than possion threshold: \n",
    "        Possion(Supp(k+1 signature), ESupp(k+1 signature)) < possion_threshold\n",
    "        Returns True is poisson value is smaller than threshold\n",
    "\n",
    "        and\n",
    "\n",
    "        Checks if support is larger than expected support: \n",
    "        Supp(k+1 signature) > ESupp(k+1 siganature)\n",
    "        ESupp = Supp(S) * width(S')\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signatue : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True, if core condition is met \n",
    "        '''\n",
    "        \n",
    "        interval = self.__diff_interval(p_signature, pplus1_signature)\n",
    "        support = self.__compute_support_sig(pplus1_signature)\n",
    "        expected_support = self.__compute_exp_support(pplus1_signature, interval)\n",
    "        base_condition = support > expected_support\n",
    "        if base_condition:\n",
    "            poisson_value = poisson.pmf(support, expected_support) \n",
    "            if poisson_value < self._poisson_threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False      \n",
    "            \n",
    "    def __merge(self, dict1, dict2):\n",
    "        '''Helper function to merge to p-signature dictonaries \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dict1: p-signature dictonary\n",
    "\n",
    "        dict2: p-signature dictronary \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res : merged p-signature containing of dict1 and dict 2\n",
    "        '''\n",
    "        \n",
    "        res = {**dict1, **dict2}\n",
    "        return res\n",
    "\n",
    "    def __a_is_subset_of_b(self, a, b):\n",
    "        '''Helper function that checks is dictionary a is a subset of dictionary b \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : dictionary\n",
    "\n",
    "        b : dictionary\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True, if a is a subset of b\n",
    "        '''\n",
    "        \n",
    "        return all((k in b and b[k]==v) for k,v in a.items())\n",
    "    \n",
    "    def __apriori_cores(self, _approx_proj_sig):\n",
    "        ''' Computes cluster cores in apriori fashion. \n",
    "        The function computes maximal p-signatures that fulfill \n",
    "        two conditions. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _approx_proj_sig : list of dictonaries\n",
    "        '''\n",
    "\n",
    "        # Loop through attributes and intervals (ignore same dimensions)\n",
    "        _cluster_cores = [_approx_proj_sig]\n",
    "\n",
    "        while _cluster_cores[-1] != []:\n",
    "            p_sig_list = []\n",
    "            for p_sig in _cluster_cores[-1]:\n",
    "                for one_sig in _approx_proj_sig:\n",
    "                  # The second criterion is to avoid double counting\n",
    "                    if list(one_sig.keys())[0] not in p_sig.keys() and list(one_sig.keys())[0] > max(list(p_sig.keys())): \n",
    "                        pplus1_sig = self.__merge(p_sig, one_sig)\n",
    "                        # Check core condition\n",
    "                        if self.__check_core_condition(p_sig, pplus1_sig):\n",
    "                            p_sig_list.append(pplus1_sig)\n",
    "            _cluster_cores.append(p_sig_list)\n",
    "        _cluster_cores.pop()\n",
    "\n",
    "        # Finds only the unique cluster cores since the above algorithm might be return the same core multiple times \n",
    "        cluster_centers_ = []\n",
    "        for p_sig_list in _cluster_cores:\n",
    "            for p_sig in p_sig_list:\n",
    "                if p_sig not in cluster_centers_:\n",
    "                    cluster_centers_.append(p_sig)\n",
    "\n",
    "        maximal_cluster_centers_ = cluster_centers_.copy()\n",
    "        # Check condition 2 for each signature (pruning to maximal cluster cores)\n",
    "        for cluster in reversed(cluster_centers_):\n",
    "            for sub_cluster in reversed(cluster_centers_):\n",
    "                if cluster != sub_cluster: \n",
    "                    if self.__a_is_subset_of_b(sub_cluster, cluster):\n",
    "                        if sub_cluster in maximal_cluster_centers_:\n",
    "                            maximal_cluster_centers_.remove(sub_cluster)\n",
    "\n",
    "        self.cluster_centers_ = maximal_cluster_centers_\n",
    "    \n",
    "    def __compute_core_set(self):\n",
    "        ''' Computes the support set for each cluster core. This is necessary for 3.3:\n",
    "        computing the projected clusters '''\n",
    "        \n",
    "        for p_signature in self.cluster_centers_:\n",
    "            dataset = np.copy(self._X)\n",
    "            for attribute in p_signature:\n",
    "                interval = p_signature[attribute]\n",
    "                remove = []\n",
    "                for i, point in enumerate(dataset):\n",
    "                    if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                        remove.append(i)\n",
    "                dataset = np.delete(dataset, remove, 0)\n",
    "            self._support_set_of_cores.append(dataset)\n",
    "    \n",
    "    \n",
    "     # Methods for 3.3: Computing projected clusters (Mahdi and Robert)\n",
    "    def __compute_fuzzy_membership_matrix(self):\n",
    "        '''Computes the fuzzy membership matrix using the support set of cores and cluster centers.\n",
    "        Then it assignes unassigned points to the closest cluster core using Mahalanobis distance'''\n",
    "        \n",
    "        n = self._X.shape[0]\n",
    "        k = len(self.cluster_centers_)\n",
    "        \n",
    "        self._fuzzy_membership_matrix = np.zeros((n, k))\n",
    "        for i in range(n):\n",
    "            for l in range(k):\n",
    "                if (any(np.array_equal(self._X[i], x) for x in  self._support_set_of_cores[l])):                       \n",
    "                    self._fuzzy_membership_matrix[i][l] = 1\n",
    "        fraction_matrix = np.sum(self._fuzzy_membership_matrix, axis=1)/k\n",
    "        fraction_matrix = fraction_matrix.reshape(n,1)\n",
    "        self._fuzzy_membership_matrix = np.multiply(self._fuzzy_membership_matrix, fraction_matrix) \n",
    "        \n",
    "        #unassigned data points are assigned to the “closest” cluster core\n",
    "        #in terms of Mahalanobis distances to means of support sets of cluster cores.\n",
    "        for i in range(n):\n",
    "            m_distance = []\n",
    "            #find rows of zero in fuzzy matrix\n",
    "            if(not np.any(self._fuzzy_membership_matrix[i])):\n",
    "                for l in range (0,len(self._support_set_of_cores)):\n",
    "                    #mean of support sets\n",
    "                    mean = sum(self._support_set_of_cores[l])/len(self._support_set_of_cores[l])\n",
    "                    #inverse covariance\n",
    "                    V = np.cov(np.array([self._X[i], mean]).T)\n",
    "                    IV = np.linalg.inv(V)\n",
    "                    #compute distance\n",
    "                    m_distance.append(distance.mahalanobis(self._X[i], mean, IV))\n",
    "                #assign to the smallest distance\n",
    "                self._fuzzy_membership_matrix[i][m_distance.index(min(m_distance))] = 1\n",
    "        \n",
    "    def __compute_hard_membership_matrix(self):\n",
    "        '''\n",
    "          For each data point compute the probability of belonging to each projected cluster using Expectation \n",
    "          Maximization(EM)algorithm.'''\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        gm = GaussianMixture(n_components=self._fuzzy_membership_matrix.shape[1]).fit(self._fuzzy_membership_matrix)\n",
    "        self.labels_ = gm.predict(self._fuzzy_membership_matrix)\n",
    "        \n",
    "        \n",
    "    # Methods for 3.4: Outlier detection (Akshey and Jonas)\n",
    "    def __outlier_detection(self):\n",
    "        '''Computes oultiers depending on mahalanobis distance\n",
    "        Sets the labels for the computed outliers to -1'''\n",
    "        \n",
    "        cluster_outlier = []\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        # Loop through cluster cores\n",
    "        for i in range(max(self.labels_)):\n",
    "            outlier_list = []\n",
    "            # Compute Covarianve matrix for each cluster\n",
    "            cluster_cov = np.cov(self._X[self.labels_==i].T)\n",
    "            # Compute inverse of covariance matrix\n",
    "            if np.linalg.cond(cluster_cov > 1000):\n",
    "                return\n",
    "            cluster_cov_inv = np.linalg.inv(cluster_cov)\n",
    "            # Computer mean of cluster core\n",
    "            cluster_mean = np.mean(self._X[self.labels_==i] , axis=0)\n",
    "            distances = []\n",
    "            # Loop through each data point in cluster core and compute mahalanobis\n",
    "            for point in self._X[self.labels_==i]:\n",
    "                maha_distance = distance.mahalanobis(point, \n",
    "                                cluster_mean, cluster_cov_inv)\n",
    "                distances.append(maha_distance)\n",
    "\n",
    "            # Compute distance threshold for outlier detection, OR IS IT JUST alpha??? \n",
    "            #threshold = chisquare (self._X[self.labels_==i].T.shape[0])\n",
    "            threshold = stats.chi2.ppf(1-self._alpha, self._X[self.labels_==i].T.shape[0])\n",
    "            #threshold = stats.chi2.ppf(self._alpha, self._X[self.labels_==i].T.shape[0])\n",
    "            outlier_indx = np.where(distances > threshold)[0].tolist()\n",
    "            # Get original data index for outlier\n",
    "            cluster_data = self._X[self.labels_==i]\n",
    "            outlier_points = cluster_data[outlier_indx]\n",
    "            for outlier_point in outlier_points:\n",
    "                indx = np.where(self._X==outlier_point)[0][0].tolist()\n",
    "                outlier_list.append(indx)\n",
    "            cluster_outlier.append(outlier_list)\n",
    "        flat_list = [item for sublist in cluster_outlier for item in sublist]\n",
    "        for i in flat_list:\n",
    "            self.labels_[i] = -1\n",
    "        \n",
    "    #sklearn-like functions       \n",
    "\n",
    "    def fit (self, X):\n",
    "        '''Fits the model according to X \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : dataset\n",
    "        '''\n",
    "        self._X = self.__normalize(X)\n",
    "        self.__compute_support()\n",
    "        self.__approximate_projection()\n",
    "        self.__apriori_cores(self.__convert_approx_proj_to_dict())\n",
    "        self.__compute_core_set()\n",
    "        \n",
    "    def predict (self, X):\n",
    "        '''Predicts labels of X according to the model and writes them to labels_, where they can be accessed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : dataset\n",
    "        '''\n",
    "        self._X = self.__normalize(X)\n",
    "        self.__compute_fuzzy_membership_matrix()\n",
    "        self.__compute_hard_membership_matrix()\n",
    "        self.__outlier_detection()\n",
    "        \n",
    "    def fit_predict (self, X):\n",
    "        self.fit (X)\n",
    "        self.predict (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p3c = P3C (1e-10)\n",
    "p3c.fit_predict (syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3c = P3C (1e-10)\n",
    "p3c.fit (test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (p3c.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (p3c._fuzzy_membership_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3c.predict (syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 1 2 2 3 3 1 2 2 2 1 1 3 1 2 1 1 1 3 3 3 2 1 1 3 2 3 2 3 3 2 2 2 2 2\n",
      " 2 3 3 2 1 3 1 1 1 0 1 1 1 2 3 2 3 1 4 1 3 2 1 3 3 2 1 2 1 3 1 2 0 1 3 3 2\n",
      " 2 3 3 3 2 2 3 1 1 3 2 1 1 3 2 1 3 1 2 3 3 2 3 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print (p3c.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 0, 32, 1, 34, 0]\n"
     ]
    }
   ],
   "source": [
    "cluster = []\n",
    "for i in range (len (p3c.cluster_centers_)):\n",
    "    count = 0\n",
    "    for entry in p3c.labels_:\n",
    "        if entry == i:\n",
    "            count += 1\n",
    "    cluster.append (count)\n",
    "\n",
    "print (cluster)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmi = normalized_mutual_info_score(p3c.labels_, syn_label)\n",
    "print (nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3c = P3C (1e-10)\n",
    "p3c.fit (test_data)\n",
    "p3c.predict (test_data)\n",
    "\n",
    "n =  p3c._X.shape[0]\n",
    "M = p3c._fuzzy_membership_matrix\n",
    "sscc = p3c._support_set_of_cores\n",
    "X = p3c._X\n",
    "\n",
    "for i in range(0,n):\n",
    "    m_distance = []\n",
    "    if(not np.any(M[i])):\n",
    "        for l in range (0,len(sscc)):\n",
    "            mean = sum(sscc[l])/len(sscc[l])\n",
    "            V = np.cov(np.array([X[i], mean]).T)\n",
    "            IV = np.linalg.inv(V)\n",
    "            m_distance.append(distance.mahalanobis(X[i], mean, IV))\n",
    "        \n",
    "        M[i][m_distance.index(min(m_distance))] = 1\n",
    "\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
