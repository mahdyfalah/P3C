{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import poisson\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture # For Expectation maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = np.genfromtxt('scale-d-10d.csv', delimiter=' ')\n",
    "data = data[:,0:9]\n",
    "\n",
    "labels = np.genfromtxt('scale-d-10d.csv', delimiter=' ',dtype=\"|U5\")\n",
    "labels = labels[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies: numpy, scipy (for scipy.stats.chisquare)\n",
    "class P3C:\n",
    "    \n",
    "    #class variables\n",
    "    _alpha = 0.001 #alpha for chi-squared-test\n",
    "\n",
    "    #Poisson threshold is the only parameter for P3C \n",
    "    def __init__(self, poisson_threshold): \n",
    "        \n",
    "        #sklearn-like attributes\n",
    "        self.labels_ = None\n",
    "        self.cluster_centers_ = None #\"cluster cores\"\n",
    "        \n",
    "        #internally used variables\n",
    "        self._poisson_threshold = poisson_threshold\n",
    "        self._support_set = []\n",
    "        self._supports = []\n",
    "        self._approx_proj = []   #nested list: 1st level: attributes, 2nd level:\n",
    "                                 #different intervals, 3rd level: start and end of inteval.\n",
    "                                 #used as interface between part 3.1 and 3.2\n",
    "    \n",
    "    \n",
    "    # Methods for 3.1: Projections of true p-signatures (Mahdi and Robert)\n",
    "    \n",
    "    def __compute_support(self, M):\n",
    "        '''Computes Supports of each bin\n",
    "        This function computes support set of each interval S and its support.\n",
    "        then assigns the values to self._support_set = [] and self._supports = []\n",
    "        SupportSet(S)= {x ∈ D | x.aj ∈ S }\n",
    "        Support(S) = |SupportSet(S)|\n",
    "        \n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "        \n",
    "        M : numpy.array \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        '''\n",
    "        n = M.shape[0] # n = number of data objects\n",
    "        attribute_number = M.shape[1] # number of attributes \n",
    "        bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "\n",
    "        for i in range(attribute_number):\n",
    "            supp_set = [[]for i in range(bin_number)] # a set containing supports of an attr\n",
    "                                                      # in different bins\n",
    "\n",
    "            aj_max = 1\n",
    "            aj_min = -1\n",
    "\n",
    "            interval_length = (aj_max - aj_min ) / bin_number\n",
    "\n",
    "            bins = np.zeros([bin_number])\n",
    "\n",
    "            for k in range(bin_number):\n",
    "                bins[k] = aj_min + (k+1)*interval_length\n",
    "\n",
    "            for j in range(n):\n",
    "                supp_set_index = 0\n",
    "                for k in range(len(bins)):\n",
    "                    if M[j,i] > bins[k]:\n",
    "                        supp_set_index += 1\n",
    "\n",
    "                \n",
    "                supp_set[supp_set_index].append(M[j])\n",
    "                \n",
    "\n",
    "\n",
    "            supp = []\n",
    "            for supp_pts in supp_set:\n",
    "                supp.append (len(supp_pts))\n",
    "            self._supports.append (supp)\n",
    "            self._support_set.append(supp_set)\n",
    "            \n",
    "            \n",
    "        # Uncomment the part below for projections\n",
    "#         for i in range (len(self._supports)):\n",
    "#             for j in range(len(self._supports[i])):\n",
    "#                 print(len(self._supports[i][j])) \n",
    "                \n",
    "        return\n",
    "    \n",
    "    def __approximate_projection(self):\n",
    "        \n",
    "        bins = np.zeros((len(self._supports), len(self._supports[0])), dtype=int)\n",
    "        for attr_number in range(len(self._supports)): #loop over all attributes\n",
    "                \n",
    "            # part 1: create bin array            \n",
    "            supp = self._supports[attr_number].copy() #make a copy of the supports of the current attribute\n",
    "            while self.__uniformity_test(supp) == False: #if not uniform find highest element\n",
    "                max_index = supp.index(max(supp))\n",
    "                supp.pop(max_index) #remove highest element from list\n",
    "                \n",
    "                i = 1 \n",
    "                while i <= max_index: #loop to adjust max_index according to previousely deleted elements\n",
    "                    max_index += bins[attr_number, i]\n",
    "                    i += 1\n",
    "                bins[attr_number, max_index]  = 1 #mark highest bin\n",
    "            \n",
    "            #part 2: create _approx_proj list from bin array\n",
    "            interval_list = [] #2d list for current attribute\n",
    "            interval = [] #current interval\n",
    "            open_interval = False\n",
    "            \n",
    "            for i in range(len(bins[attr_number])):\n",
    "                if open_interval == False: #open new interval\n",
    "                    if bins[attr_number, i] == 1:\n",
    "                        interval.append(i)\n",
    "                        open_interval = True\n",
    "                if open_interval == True: #close current interval\n",
    "                    if bins[attr_number, i] == 0:\n",
    "                        interval.append(i)\n",
    "                        interval_list.append (interval)\n",
    "                        interval = []\n",
    "                        open_interval = False\n",
    "                    if (i == len(bins[attr_number])-1) and (bins[attr_number, i] == 1): #last bin marked 1\n",
    "                        interval.append (len(bins[attr_number]))\n",
    "                        interval_list.append (interval)                \n",
    "        \n",
    "            self._approx_proj.append (interval_list)\n",
    "        \n",
    "    def __uniformity_test(self, attr):\n",
    "        if chisquare(attr)[0] < self._alpha:\n",
    "            #print (\"uniform\")\n",
    "            return True\n",
    "        #print (\"non-uniform\")\n",
    "        return False  \n",
    "    \n",
    "    # Methods for 3.3: Cluster Cores (Akshey and Jonas)\n",
    "    \n",
    "    def __compute_support_sig(p_signature, dataset):\n",
    "        '''Computes support for p-signature\n",
    "        This function computes the support by removing data points\n",
    "        that do not lie in any of the intervals of the given p-signature\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "        \n",
    "        dataset : numpy.ndarray \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data.shape[0] : number of points in p-signature\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        data = np.copy(dataset)\n",
    "        for attribute in p_signature:\n",
    "            interval = p_signature[attribute]\n",
    "            remove = []\n",
    "            for i, point in enumerate(data):\n",
    "                if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                    remove.append(i)\n",
    "            data = np.delete(data, remove, 0)\n",
    "        return data.shape[0]\n",
    "    \n",
    "    \n",
    "    def __compute_exp_support(p_signature, interval, data):\n",
    "        ''' Computes expected support for a p-signature\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "            \n",
    "        interval : list with start and end value of interval\n",
    "        \n",
    "        data : normalized np.ndarray\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        support * width : \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        support = __compute_support_sig(p_signature, data)\n",
    "        width = abs(interval[0] - interval[1])\n",
    "        return support*width\n",
    "    \n",
    "    \n",
    "    def __diff_interval(p_signature, pplus1_signature):\n",
    "        '''Helper function to compute difference in interval for two p-signatures.\n",
    "           Used for possion threshold\n",
    "           \n",
    "        Parameters\n",
    "        ---------- \n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "         \n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "           \n",
    "        Returns\n",
    "        -------\n",
    "        interval : \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        diff = list(set(pplus1_signature) - set(p_signature))\n",
    "        interval = pplus1_signature[diff[0]]\n",
    "        return interval\n",
    "\n",
    " \n",
    "    def __check_core_condition(p_signature, pplus1_signature, dataset, threshold=1e-20):\n",
    "        ''' Checks if probability is smaller than possion threshold: \n",
    "        Possion(Supp(k+1 signature), ESupp(k+1 signature)) < possion_threshold\n",
    "        Returns True is poisson value is smaller than threshold\n",
    "\n",
    "        and\n",
    "\n",
    "        Checks if support is larger than expected support: \n",
    "        Supp(k+1 signature) > ESupp(k+1 siganature)\n",
    "        ESupp = Supp(S) * width(S')\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signatue : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "        \n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "        \n",
    "        dataset : numpy.ndarray\n",
    "        \n",
    "        threshold : poisson_threshold -> defined by user. default: 1e-20\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        true/false : \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        interval = diff_interval(p_signature, pplus1_signature)\n",
    "        support = __compute_support_sig(pplus1_signature, dataset)\n",
    "        expected_support = __compute_exp_support(pplus1_signature, interval, dataset)\n",
    "        base_condition = support > expected_support\n",
    "        if base_condition:\n",
    "            possion_value = poisson.pmf(support, expected_support) \n",
    "            if poisson_value < threshold:\n",
    "                return True\n",
    "    \n",
    "    \n",
    "    def __apriori_cores(approx_proj, supports):\n",
    "        ''' Computes cluster cores in apriori fashion. \n",
    "        The function computes maximal p-signatures that fulfill \n",
    "        two conditions. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        approx_proj :\n",
    "\n",
    "        supports : \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_p_signatures : \n",
    "\n",
    "        '''\n",
    "        \n",
    "          # Loop through attributes and intervals (ignore same dimensions)\n",
    "\n",
    "            # Compute k+1 signatures from valid k signatures\n",
    "\n",
    "            # Check condition 1 for each signature (check_supp_expected_supp())\n",
    "\n",
    "            # Check condition 2 for each signature\n",
    "\n",
    "            # Prune away infrequent k+1 signatures\n",
    "\n",
    "          # Select maximal p-signatures\n",
    "\n",
    "        pass\n",
    "    \n",
    "     # Methods for 3.3: Computing projected clusters (Manju)\n",
    "    def __fuzzy_membership_matrix(cluster_core_i,data): \n",
    "        '''\n",
    "        Refines the cluster cores into projected clusters\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_core_i:\n",
    "        \n",
    "        data:\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fuzzy_membership_matrix:\n",
    "        '''\n",
    "        fuzzy_membership_matrix=[]\n",
    "        for i in range(1,n):\n",
    "            for l in range(1,k):\n",
    "                if (i in data and l in support_set(cluster_core_i)):\n",
    "                    if(i not in support_set(cluster_core_i)): \n",
    "                        fuzzy_membership_matrix=0\n",
    "                       # unassigned_datapoints=cluster_core_i.append(i)\n",
    "                    elif (i in support_set(cluster_core_i)):\n",
    "                        fuzzy_membership_matrix=(1/support_set(cluster_core_i))[data]\n",
    "        \n",
    "        return fuzzy_membership_matrix   \n",
    "    \n",
    "                \n",
    "    def __probability_of_datapoint(fuzzy_membership_matrix,max_iterations):\n",
    "        '''\n",
    "          For each data point compute the probability of belonging to each projected cluster using Expectation \n",
    "          Maximization(EM)algorithm.\n",
    "          Parameters\n",
    "          ----------\n",
    "          fuzzy_membership_matrix:\n",
    "          \n",
    "          max_iterations:\n",
    "          \n",
    "          Returns\n",
    "          -------\n",
    "          probability_matrix:\n",
    "        \n",
    "        '''\n",
    "        # initialise EM with fuzzy_membership_matrix.cluster members have shorter mahalanobis distances to cluster\n",
    "        # means than non-cluster members\n",
    "        max_iterations=10\n",
    "        gaussian_mixture = GaussianMixture(n_components=2, covariance_type='full').fit(fuzzy_membership_matrix)\n",
    "        gaussian_mixture.means_\n",
    "        gaussian_mixture.fit()\n",
    "        label=gaussian_mixture.predict(fuzzy_membership_matrix)\n",
    "        \n",
    "        return gaussian_mixture\n",
    "        \n",
    "\n",
    "\n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit (self, X):\n",
    "        #all the method calls of 3.1 and 3.2 we have to implement go here...\n",
    "        self.__compute_support(X)\n",
    "        self.__approximate_projection()\n",
    "        \n",
    "        #self.cluster_center_ = ... #used as interface between part 3.2. and 3.3\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def predict (self, X):\n",
    "        pass #remove pass when implementing predict (X)\n",
    "        \n",
    "        #all the method calls of 3.3, 3.4 and 3.5 we have to implement go here...\n",
    "        \n",
    "        #self.labels_ = #final result of the algorithm.\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit_predict (self, X):\n",
    "        self.fit (X)\n",
    "        self.predict (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 14]], [[1, 14]], [[0, 1], [2, 14]], [[0, 13]], [[1, 14]], [[1, 13]], [[1, 14]], [[2, 14]], [[1, 14]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahdyfalah/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:6125: RuntimeWarning: invalid value encountered in true_divide\n",
      "  terms = (f_obs.astype(np.float64) - f_exp)**2 / f_exp\n",
      "/Users/mahdyfalah/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:6118: RuntimeWarning: Mean of empty slice.\n",
      "  f_exp = f_obs.mean(axis=axis, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "p3c = P3C (10)\n",
    "p3c.fit (data)\n",
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_add):\n",
    "    normalized_data = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    csv = np.genfromtxt(data_add, delimiter=' ')\n",
    "    num_rows = csv.shape[0]\n",
    "    num_cols = csv.shape[1]\n",
    "    data = csv[:,0:num_cols-1]\n",
    "\n",
    "    normalized_data = preprocessing.normalize(data)\n",
    "    labels = np.loadtxt(data_add, delimiter=' ', dtype=str, usecols = range(num_cols-1,num_cols))\n",
    "    \n",
    "    return normalized_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 1], [2, 14]], [[0, 14]], [[0, 14]], [[0, 1], [2, 13]], [[0, 14]], [[0, 14]], [[1, 14]], [[0, 14]], [[0, 1], [2, 13]], [[0, 14]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahdyfalah/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:6125: RuntimeWarning: invalid value encountered in true_divide\n",
      "  terms = (f_obs.astype(np.float64) - f_exp)**2 / f_exp\n",
      "/Users/mahdyfalah/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:6118: RuntimeWarning: Mean of empty slice.\n",
      "  f_exp = f_obs.mean(axis=axis, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "normalized_data, labels = preprocess('scale-d-10d.csv')\n",
    "p3c = P3C (10)\n",
    "p3c.fit (normalized_data)\n",
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
