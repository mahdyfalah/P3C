{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import poisson\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture # For Expectation maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = np.genfromtxt('scale-d-10d.csv', delimiter=' ')\n",
    "data = data[:300,:10]\n",
    "\n",
    "labels = np.genfromtxt('scale-d-10d.csv', delimiter=' ',dtype=\"|U5\")\n",
    "labels = labels[:300,10]\n",
    "\n",
    "for i in range (labels.shape[0]):\n",
    "    labels[i] = int(labels[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array ([[0.15, 0.30, 0.00, 0.80], #c1\n",
    "                       [0.12, 0.29, 0.35, 0.69], #c1\n",
    "                       [0.00, 0.45, 0.61, 0.27], #c1\n",
    "                       [0.39, 0.00, 1.00, 0.10], #c2\n",
    "                       [0.59, 0.80, 0.10, 1.00], #c2\n",
    "                       [0.54, 0.90, 0.29, 0.63], #c2           \n",
    "                       [0.69, 1.00, 0.59, 0.28],\n",
    "                       [1.00, 0.60, 0.81, 0.00],\n",
    "                       [0.20, 0.27, 0.39, 0.97], #c1\n",
    "                       [0.03, 0.47, 0.08, 0.57]])#c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = np.genfromtxt ('housing.csv')\n",
    "#house_data = house_data[:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Replaced by the method P3C.__normalize\\n# MinMaxScaler did not work properly!\\n# https://datascience.stackexchange.com/questions/38004/minmaxscaler-returned-values-greater-than-one\\ndef rescale(data, new_min=0, new_max=1):\\n    Rescale the data to be within the range [new_min, new_max]\\n    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Replaced by the method P3C.__normalize\n",
    "# MinMaxScaler did not work properly!\n",
    "# https://datascience.stackexchange.com/questions/38004/minmaxscaler-returned-values-greater-than-one\n",
    "def rescale(data, new_min=0, new_max=1):\n",
    "    Rescale the data to be within the range [new_min, new_max]\n",
    "    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies: numpy, scipy (for scipy.stats.chisquare), sklearn (for sklearn.preprocessing.MinMaxScaler)\n",
    "class P3C:\n",
    " # General class functionality (Robert)\n",
    "    \n",
    "    #class variables\n",
    "    _alpha = 0.001 #alpha for chi-squared-test\n",
    "    _EM_iter = 10 #iterations for EM\n",
    "\n",
    "    #Poisson threshold is the only parameter for P3C \n",
    "    def __init__(self, poisson_threshold): \n",
    "        \n",
    "        #sklearn-like attributes\n",
    "        self.labels_ = [] \n",
    "        self.cluster_centers_ = None #\"cluster cores\"\n",
    "        \n",
    "        #internally used variables\n",
    "        self._X = None\n",
    "        self._poisson_threshold = poisson_threshold\n",
    "        self._support_set = []\n",
    "        self._supports = []\n",
    "        self._approx_proj = []\n",
    "        self._support_set_of_cores = []\n",
    "        self._fuzzy_membership_matrix = None\n",
    "        self._hard_membership_matrix = None #np.array: dimension: number of samples x number of clusters\n",
    "        \n",
    "    \n",
    "    def __convert_matrix_to_labels (self):\n",
    "        \"\"\"Converts membership matrix to labels\"\"\"\n",
    "        \n",
    "        for sample in self._membership_matrix:\n",
    "            for entry in range(sample.size):\n",
    "                if sample[entry] == 1:\n",
    "                    self.labels_.append(entry)\n",
    "    \n",
    "    # Methods for 3.1: Projections of true p-signatures (Mahdi and Robert)\n",
    "\n",
    "    def __normalize(self, X):\n",
    "        '''Normalizes the data featurewise\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        X : numpy.array of the input data \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array of the normalized data\n",
    "        \n",
    "        '''\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "    \n",
    "    def __uniformity_test(self, attr):\n",
    "        '''Uses the chi-squared test to determine if the attribute is uniformly disributed,\n",
    "        using the class variable self._alpha as a certrainty-threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        attr: input list to be tested for uniformity \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True, if input data is uniform\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if chisquare(attr)[0] < self._alpha:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def __compute_support(self):\n",
    "        '''Computes Supports of each bin\n",
    "        This function computes support set of each interval S and its support.\n",
    "        then assigns the values to self._support_set = [] and self._supports = []\n",
    "        SupportSet(S)= {x ∈ D | x.aj ∈ S }\n",
    "        Support(S) = |SupportSet(S)|\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        M : numpy.array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        '''\n",
    "        n = self._X.shape[0] # n = number of data objects\n",
    "        attribute_number = self._X.shape[1] # number of attributes \n",
    "        bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "\n",
    "\n",
    "        for i in range(attribute_number):\n",
    "            # support set of each interval S for attribiute i\n",
    "            supp_set = [[]for i in range(bin_number)]\n",
    "\n",
    "            interval_length = 1 / bin_number\n",
    "\n",
    "            # calculate in which bin should the point be placed based on attribute i\n",
    "            for j in range(n):\n",
    "                supp_set_index = math.floor(self._X[j,i]/interval_length)\n",
    "\n",
    "                if supp_set_index == len(supp_set):\n",
    "                    supp_set_index-=1 \n",
    "\n",
    "                supp_set[supp_set_index].append(self._X[j,:])\n",
    "\n",
    "            self._support_set.append(supp_set)\n",
    "            \n",
    "\n",
    "        self._supports = [[] for i in range(len(self._support_set))]\n",
    "        for i in range(len(self._support_set)):\n",
    "            for j in range(len(self._support_set[i])):\n",
    "                self._supports[i].append(len(self._support_set[i][j]))\n",
    "\n",
    "        return \n",
    "    \n",
    "    def __approximate_projection(self):\n",
    "        \n",
    "        '''Finds the bins that violate uniform distribution from the data stored in self._supports and\n",
    "        storest them in the numpy.array bins, with 1 for non-uniformity and 0 for uniformity. It then \n",
    "        assignes the intervals to self._approx_proj\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        '''\n",
    "        \n",
    "        bins = np.zeros((len(self._supports), len(self._supports[0])), dtype=int)\n",
    "        for attr_number in range(len(self._supports)): #loop over all attributes\n",
    "            #print (\"attr_nr =\", attr_number)\n",
    "                            \n",
    "            supp = self._supports[attr_number].copy() #make a copy of the supports of the current attribute\n",
    "            while self.__uniformity_test(supp) == False: #if not uniform find highest element\n",
    "                max_index = supp.index(max(supp))\n",
    "                supp.pop(max_index) #remove highest element from list\n",
    "                \n",
    "                i = 0 \n",
    "                while i <= max_index: #loop to adjust max_index according to previousely deleted elements\n",
    "                    max_index += bins[attr_number, i]\n",
    "                    i += 1\n",
    "                bins[attr_number, max_index]  = 1 #mark highest bin\n",
    "            \n",
    "            #print (\"current bin: \", bins[attr_number])\n",
    "            \n",
    "            interval_list = [] #2d list for current attribute\n",
    "            interval = [] #current interval\n",
    "            open_interval = False\n",
    "\n",
    "            for i in range(len(bins[attr_number])):\n",
    "                if open_interval == False: #open new interval\n",
    "                    if bins[attr_number, i] == 1:\n",
    "                        interval.append(i/len(bins[attr_number]))\n",
    "                        open_interval = True\n",
    "                if open_interval == True: #close current interval\n",
    "                    if bins[attr_number, i] == 0:\n",
    "                        interval.append(i/len(bins[attr_number]))\n",
    "                        interval_list.append (interval)\n",
    "                        interval = []\n",
    "                        open_interval = False\n",
    "                    if (i == len(bins[attr_number])-1) and (bins[attr_number, i] == 1): #last bin marked 1\n",
    "                        interval.append ((i+1)/len(bins[attr_number]))\n",
    "                        interval_list.append (interval)                \n",
    "\n",
    "            self._approx_proj.append (interval_list)\n",
    "            \n",
    "    \n",
    "    # Methods for 3.2: Cluster Cores (Akshey and Jonas)\n",
    "    \n",
    "    def __convert_approx_proj_to_dict(self):\n",
    "        ''' Converts _approx_proj from 3.2 to a dictonary\n",
    "        This is necessary to compute cluster cores with apriori\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self._approx_proj : three nested lists (# attribut/# interval/ start and end of interval)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _approx_proj_sig : list of dictonaries, each dictonary is a projection e.g {0: [0.1, 0.2]}\n",
    "\n",
    "        '''\n",
    "        _approx_proj_sig = []\n",
    "        for attribute, row in enumerate(self._approx_proj):\n",
    "            for interval in row:\n",
    "                _approx_proj_sig.append({attribute:interval})\n",
    "                \n",
    "        return _approx_proj_sig\n",
    "    \n",
    "    \n",
    "    def __compute_support_sig(self, p_signature):\n",
    "        '''Computes support for p-signature\n",
    "        This function computes the support by removing data points\n",
    "        that do not lie in any of the intervals of the given p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        dataset : numpy.ndarray \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data.shape[0] : number of points in p-signature\n",
    "\n",
    "        '''\n",
    "\n",
    "        data = np.copy(self._X)\n",
    "        for attribute in p_signature:\n",
    "            interval = p_signature[attribute]\n",
    "            remove = []\n",
    "            for i, point in enumerate(data):\n",
    "                if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                    remove.append(i)\n",
    "            data = np.delete(data, remove, 0)\n",
    "\n",
    "        return data.shape[0]\n",
    "\n",
    "    \n",
    "    def __compute_exp_support(self, p_signature, interval):\n",
    "        ''' Computes expected support for a p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        interval : list with start and end value of interval\n",
    "\n",
    "        data : normalized np.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        support * width : \n",
    "\n",
    "        '''\n",
    "\n",
    "        support = self.__compute_support_sig(p_signature)\n",
    "        width = abs(interval[0] - interval[1])\n",
    "\n",
    "        return support*width\n",
    "    \n",
    "    \n",
    "    def __diff_interval(self, p_signature, pplus1_signature):\n",
    "        '''Helper function to compute difference in interval for two p-signatures.\n",
    "           Used for possion threshold\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interval : \n",
    "\n",
    "        '''\n",
    "\n",
    "        diff = list(set(pplus1_signature) - set(p_signature))\n",
    "        interval = pplus1_signature[diff[0]]\n",
    "        \n",
    "        return interval\n",
    "\n",
    "\n",
    "\n",
    "    def __check_core_condition(self, p_signature, pplus1_signature):\n",
    "        ''' Checks if probability is smaller than possion threshold: \n",
    "        Possion(Supp(k+1 signature), ESupp(k+1 signature)) < possion_threshold\n",
    "        Returns True is poisson value is smaller than threshold\n",
    "\n",
    "        and\n",
    "\n",
    "        Checks if support is larger than expected support: \n",
    "        Supp(k+1 signature) > ESupp(k+1 siganature)\n",
    "        ESupp = Supp(S) * width(S')\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signatue : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        dataset : numpy.ndarray\n",
    "\n",
    "        threshold : poisson_threshold -> defined by user. default: 1e-20\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        true/false : \n",
    "\n",
    "        '''\n",
    "        \n",
    "        interval = self.__diff_interval(p_signature, pplus1_signature)\n",
    "        support = self.__compute_support_sig(pplus1_signature)\n",
    "        expected_support = self.__compute_exp_support(pplus1_signature, interval)\n",
    "        base_condition = support > expected_support\n",
    "        if base_condition:\n",
    "            poisson_value = poisson.pmf(support, expected_support) \n",
    "            if poisson_value < self._poisson_threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            \n",
    "    def __merge(self, dict1, dict2):\n",
    "        '''Helper function to merge to p-signature dictonaries \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dict1: p-signature dictonary\n",
    "\n",
    "        dict2: p-signature dictronary \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res : merged p-signature containing of dict1 and dict 2\n",
    "\n",
    "        '''\n",
    "        res = {**dict1, **dict2}\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def __a_is_subset_of_b(self, a,b):\n",
    "        '''Helper function that checks is dictionary a is a subset of dictionary b \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : dictionary\n",
    "\n",
    "        b : dictionary\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool : truth value\n",
    "\n",
    "        '''\n",
    "        return all((k in b and b[k]==v) for k,v in a.items())\n",
    "    \n",
    "    \n",
    "    def __apriori_cores(self, _approx_proj_sig):\n",
    "        ''' Computes cluster cores in apriori fashion. \n",
    "        The function computes maximal p-signatures that fulfill \n",
    "        two conditions. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _approx_proj_sig : list of dictonaries\n",
    "\n",
    "        data: dataset np.ndarray\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_centers_ : list of p-signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Loop through attributes and intervals (ignore same dimensions)\n",
    "        _cluster_cores = [_approx_proj_sig]\n",
    "\n",
    "        while _cluster_cores[-1] != []:\n",
    "            p_sig_list = []\n",
    "            for p_sig in _cluster_cores[-1]:\n",
    "                for one_sig in _approx_proj_sig:\n",
    "                  ### The second criterion is to avoid double counting\n",
    "                  ### In case this second criteria gives errors we could make a set of the p-signatures before checking the core condition\n",
    "                    if list(one_sig.keys())[0] not in p_sig.keys() and list(one_sig.keys())[0] > max(list(p_sig.keys())): \n",
    "                        pplus1_sig = self.__merge(p_sig, one_sig)\n",
    "                        ### Check core condition\n",
    "                        if self.__check_core_condition(p_sig, pplus1_sig):\n",
    "                            p_sig_list.append(pplus1_sig)\n",
    "            _cluster_cores.append(p_sig_list)\n",
    "        _cluster_cores.pop()\n",
    "\n",
    "        # Finds only the unique cluster cores since the above algorithm might be return the same core multiple times \n",
    "        cluster_centers_ = []\n",
    "        for p_sig_list in _cluster_cores:\n",
    "            for p_sig in p_sig_list:\n",
    "                if p_sig not in cluster_centers_:\n",
    "                    cluster_centers_.append(p_sig)\n",
    "\n",
    "        maximal_cluster_centers_ = cluster_centers_.copy()\n",
    "        # Check condition 2 for each signature (pruning to maximal cluster cores)\n",
    "        for cluster in reversed(cluster_centers_):\n",
    "            for sub_cluster in reversed(cluster_centers_):\n",
    "                if cluster != sub_cluster: \n",
    "                    if self.__a_is_subset_of_b(sub_cluster, cluster):\n",
    "                        if sub_cluster in maximal_cluster_centers_:\n",
    "                            maximal_cluster_centers_.remove(sub_cluster)\n",
    "\n",
    "\n",
    "        self.cluster_centers_ = maximal_cluster_centers_\n",
    "\n",
    "    \"\"\"\n",
    "    def __compute_core_support(self, cores_p_signatures):\n",
    "        ''' Computes the support number for each cluster core.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cores_p_signatures : list of core signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cores_support_number : list of support numbers for each cluster core\n",
    "\n",
    "        '''\n",
    "        cores_support_number = []\n",
    "        for p_sig in cores_p_signatures:\n",
    "            cores_support_number.append(__compute_support_sig(p_sig))\n",
    "        \n",
    "        return cores_support_number\n",
    "    \"\"\"\n",
    "    \n",
    "    def __compute_core_set(self):\n",
    "        ''' Computes the support set for each cluster core. This is necessary for 3.3:\n",
    "        computing the projected clusters \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cores_p_signatures : list of core signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cores_set : list of support sets for each cluster core\n",
    "\n",
    "        '''\n",
    "        \n",
    "        for p_signature in self.cluster_centers_:\n",
    "            dataset = np.copy(self._X)\n",
    "            for attribute in p_signature:\n",
    "                interval = p_signature[attribute]\n",
    "                remove = []\n",
    "                for i, point in enumerate(dataset):\n",
    "                    if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                        remove.append(i)\n",
    "                dataset = np.delete(dataset, remove, 0)\n",
    "            self._support_set_of_cores.append(dataset)\n",
    "    \n",
    "     # Methods for 3.3: Computing projected clusters (Manju & Mahdi)\n",
    "    def __compute_fuzzy_membership_matrix(self):\n",
    "        '''\n",
    "        Refines the cluster cores into projected clusters\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_core_i:\n",
    "        \n",
    "        data:\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fuzzy_membership_matrix: \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        n = self._X.shape[0]\n",
    "        k = len(self.cluster_centers_)\n",
    "        \n",
    "        self._fuzzy_membership_matrix = np.zeros((n, k))\n",
    "        for i in range(0,n):\n",
    "            for l in range(0,k):\n",
    "                if (any(np.array_equal(self._X[i], x) for x in  self._support_set_of_cores[l])):\n",
    "#                     mil is equal to the fraction of clusters cores that \n",
    "#                     contain data point i in their support set, \n",
    "#                     if i is in the support set of cluster core l.\n",
    "#                     fuzzy[i][l] = cluster cores with i / cluster cores\n",
    "                    \"\"\"\n",
    "                    counter = 0\n",
    "    \n",
    "                    \n",
    "                    for j in range(len(self._support_set_of_cores)):\n",
    "                        if (any(np.array_equal(self._X[i], x) for x in  self._support_set_of_cores[j])):\n",
    "                            counter += 1\n",
    "                        \n",
    "                    \"\"\"\n",
    "                    self._fuzzy_membership_matrix[i][l] = 1\n",
    "        \n",
    "        #pre_matrix = np.random.randint(2, size=(100,5)) ###100 objects, 5 cluster cores\n",
    "        fraction_matrix = np.sum(self._fuzzy_membership_matrix, axis=1)/k\n",
    "        fraction_matrix = fraction_matrix.reshape(n,1)\n",
    "        self._fuzzy_membership_matrix = np.multiply(self._fuzzy_membership_matrix, fraction_matrix) \n",
    "        \n",
    "    \n",
    "    def __compute_hard_membership_matrix(self):\n",
    "        '''\n",
    "          For each data point compute the probability of belonging to each projected cluster using Expectation \n",
    "          Maximization(EM)algorithm.\n",
    "          Parameters\n",
    "          ----------\n",
    "          fuzzy_membership_matrix:\n",
    "          \n",
    "          max_iterations:\n",
    "          \n",
    "          Returns\n",
    "          -------\n",
    "          membership_matrix:\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        gm = GaussianMixture(n_components=self._fuzzy_membership_matrix.shape[1]).fit(self._fuzzy_membership_matrix)\n",
    "        self.labels_ = gm.predict(self._fuzzy_membership_matrix)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit (self, X):\n",
    "        self._X = self.__normalize(X)\n",
    "        self.__compute_support()\n",
    "        self.__approximate_projection()\n",
    "        self.__apriori_cores(self.__convert_approx_proj_to_dict())\n",
    "        self.__compute_core_set()\n",
    "        \n",
    "        \n",
    "        #self.__convert_matrix_to_labels()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.cluster_center_ = ... #used as interface between part 3.2. and 3.3\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def predict (self, X):\n",
    "        self.__compute_fuzzy_membership_matrix()\n",
    "        self.__compute_hard_membership_matrix()\n",
    "        \n",
    "        #all the method calls of 3.3, 3.4 and 3.5 we have to implement go here...\n",
    "        \n",
    "        #self.labels_ = #final result of the algorithm.\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit_predict (self, X):\n",
    "        self.fit (X)\n",
    "        self.predict (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p3c = P3C (1e-10)\n",
    "p3c.fit (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 31, 74, 107, 55, 12, 6, 4], [2, 15, 36, 52, 73, 76, 31, 14, 1], [4, 25, 68, 56, 65, 60, 12, 7, 3], [33, 100, 35, 37, 31, 37, 16, 6, 5], [3, 7, 41, 99, 101, 32, 12, 4, 1], [2, 4, 35, 86, 97, 51, 16, 6, 3], [1, 2, 11, 25, 101, 108, 41, 6, 5], [3, 37, 40, 60, 78, 53, 21, 6, 2], [1, 3, 9, 78, 98, 78, 26, 3, 4], [2, 32, 117, 51, 39, 32, 17, 6, 4]]\n"
     ]
    }
   ],
   "source": [
    "print (p3c._supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.0, 0.8888888888888888]], [[0.0, 0.8888888888888888]], [[0.0, 0.8888888888888888]], [[0.0, 0.8888888888888888]], [[0.0, 0.8888888888888888]], [[0.1111111111111111, 1.0]], [[0.1111111111111111, 1.0]], [[0.0, 0.8888888888888888]], [[0.1111111111111111, 1.0]], [[0.1111111111111111, 1.0]]]\n"
     ]
    }
   ],
   "source": [
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: [0.0, 0.8888888888888888]}, {1: [0.0, 0.8888888888888888]}, {2: [0.0, 0.8888888888888888]}, {3: [0.0, 0.8888888888888888]}, {4: [0.0, 0.8888888888888888]}, {5: [0.1111111111111111, 1.0]}, {6: [0.1111111111111111, 1.0]}, {7: [0.0, 0.8888888888888888]}, {8: [0.1111111111111111, 1.0]}, {9: [0.1111111111111111, 1.0]}]\n"
     ]
    }
   ],
   "source": [
    "print (p3c.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len (p3c._support_set_of_cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3c.predict (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len (p3c._support_set_of_cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 6 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 3 4]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print (p3c.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.  0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0. ]\n",
      " [0.  0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.  0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.9 0.  0.9 0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.  0.9 0.9]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      " [0.9 0.9 0.9 0.  0.9 0.9 0.9 0.9 0.9 0.9]\n",
      " [0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.  0.9 0.9]]\n"
     ]
    }
   ],
   "source": [
    "print (p3c._fuzzy_membership_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len (p3c.cluster_centers_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 6 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 3 4]\n"
     ]
    }
   ],
   "source": [
    "print (p3c.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[280, 4, 3, 5, 2, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "cluster = []\n",
    "for i in range (len (p3c.cluster_centers_)):\n",
    "    count = 0\n",
    "    for entry in p3c.labels_:\n",
    "        if entry == i:\n",
    "            count += 1\n",
    "    cluster.append (count)\n",
    "\n",
    "print (cluster)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print (len (p3c.cluster_centers_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
