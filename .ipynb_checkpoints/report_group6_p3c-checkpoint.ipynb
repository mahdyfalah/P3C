{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3C - A Robust Projected Clustering Algorithm\n",
    "\n",
    "### Group 6\n",
    "### Robert Brunnsteiner, Mohammad Mahdi Fallah, Akshey Kumar, Manju Mariam Mathew*, Jan-Jonas Schumacher\n",
    "### VU Data Mining (WS 2021/22)\n",
    "*left the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from p3c import P3C\n",
    "from elki import run_elki_P3C, run_elki_subclu, run_elki_dbscan, run_elki_kmeans, run_elki_predecon\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Overview of the Algorithm</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P3C is a projected clustering algorithm that aims to cluster datapoints according to a subset of attributes. It should be able to detect arbitrarily-oriented, low-dimensional projected clusters in high-dimensional spaces. It consists of the following five parts:\n",
    "* Finding the projections of the true p-signatures using datapoint assignment into bins.\n",
    "* Finding cluster cores using a sophisticated Apriori-like search.\n",
    "* Computing projected clusters (using methods including Mahalanobis distance and Expectation Maximization).\n",
    "* Outlier detection using statistical methods.\n",
    "* Relevant Attributes Detection (i.e., refinement of clusters).\n",
    "\n",
    "All those parts are thoroughly discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Class design of our Implementation</u>\n",
    "\n",
    "We implemented our algorithm by creating a calss (P3C) that works similar to the ones in scikit-learn. From the outside, a class object can be created by using the Poisson threshold as the only parameter. Then wie can fit a dataset using P3C.fit(X), predict labels using P3C.predict(X) or doing both together by calling P3C.fit_predict (X). All other methods are only used internally and start with a double underscore. After calling P3C.fit(X), cluster_centers_ contains the calculated cluster centers and after calling P3C.predict(X) labels_ contains the labels for each datapoint. All other variables are only used internally and start with a single underscore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>3.1 Projections of true p-signatures</u>\n",
    "\n",
    "We start the algorithm by computing the support set of each attribute on the normalized dataset in function compute_support(self).\n",
    "Support set of an interval s is defined as below:\n",
    "    \n",
    "    SuppSet(S) := {x ∈ D|x.aj ∈ S}\n",
    "\n",
    "This is possible by spliting each attribute to same number of bins with equal size.it is recommended that bin number = 1+log2(n) with n being number of data points.\n",
    "\n",
    "Then for each attribute we look through all the data points, and assign data points to bins according to their value in said attribute.(a data point belongs to a bin if its attribute is in range of that bin)\n",
    "\n",
    "The result is the support set of the whole data. what we need for the rest of the algorithm is support which is defined by size of each bin in support set of that attribute, which is stored in self._supports.\n",
    "\n",
    "    Supp(S) := |SuppSet(S)|\n",
    "\n",
    "We continue the algorithm with approximate_projection(self) in order to identify attributes with uniform distribution, and for non uniform attributes, bins with unusual high supports. From scipy.stats we use chisquare to check for uniformity in uniformity_test(self, attr)\n",
    "\n",
    "intervals deemed uniform will be stored in and np.array with value 0, and non uniform intervals will be stored as 1.\n",
    "\n",
    "The intervals are then assigned to self.\\_approx\\_proj."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>3.2 Cluster Cores</u>\n",
    "\n",
    "The challenge in this part of the algorithm is to compute the p-signatures, that represent the cluster cores. The problem lies in the fact that the number of possible p-signature combinations is high in practical applications. We approach this issue by computing the cores in a Apriori fashion and exploiting the downward closure property.  The idea is to construct an outer and inner loop for the approximate projections. The first loop represents iterations over p-signatures and the outer loop iterates through the approximate projections or 1-signatures. Then, we must check if we can create a higher dimensional p+1-\n",
    "signature depending on the core conditions described in the following and taken from the original paper:\n",
    "\n",
    "\"A p-signature S = {S1, . . . ,Sp} together with its support set SuppSet(S) is called a cluster core, if:\n",
    "1. For any q-signature Q ⊆ S, q = 1,p−1, and any interval S′ ∈ S \\ Q, it holds that:\n",
    "        Supp(Q ∪ {S′}) > ESupp (Q ∪ {S′}|Q), \n",
    "        and Poisson(Supp(Q ∪ {S′}), ESupp(Q ∪{S′}|Q)) < Poisson threshold\n",
    "\n",
    "2. For any interval S′ not in S, it holds that \n",
    "        Supp(S ∪ {S′}) ≤ ESupp (S ∪ {S′}|S), \n",
    "        or Poisson(Supp(S ∪ {S′}), ESupp(S∪ {S′}|S) ≥ Poisson threshold.\"\n",
    "\n",
    "ESupp is the expected support of the p+1 signature R=S u {S'}|S and can be computed by Supp(S) * width(S'), where the width is the length of the interval. \n",
    "The Poisson probability is used to quantify how likely Supp(R) is with respect to ESupp(R). More precisely, a less likely Supp(R) the higher the probability that S' represents the same projected cluster as S.  \n",
    "\n",
    "In our function \\__apriori_cores(self, _approx_proj_sig) the first if statement checks if the p-signature and the chosen 1-signature do not share the same dimension, avoids double counting by ignoring previously considered signatures and constructs the p+1-signature by merging the p-signature and a 1-signature with the function self.__merge(p_sig, one_sig).\n",
    "\n",
    "The second if statement checks the first part of the above described core condition by utilizing the function self.__check_core_condition(self, p-signature, pplus1_signature).\n",
    "\n",
    "\n",
    "The next section of the algorithm finds only the unique cluster cores since the above might return the same core multiple times. Finally, we implemented the second part of the core conditions, which states that the cluster cores should be maximal. This is achieved by pruning cluster cores that are subsets of higher dimensional cluster cores. Th maximal cluster cores can be accessed by the instance variable 'self.cluster_centers_'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>3.3 Computing projected clusters</u>\n",
    "\n",
    "In this part using cluster cores and their support sets (which have been computed in part 3.2), we create a matrix to store which point belongs to which cluster core, this matrix has the following shape : n (number of data points) * k (number of clusters).\n",
    "\n",
    "The function is implemented in compute_fuzzy_membership_matrix(self). We start by assignment of fuzzy_membership_matrix to a np.array of zeros of size n*k.\n",
    "\n",
    "For each data point we check to wich cluster it belongs and we set that to 1. Later we change 1s of each row to fraction of clusters that have this data point, by summing each row and dividing rows by k (number of clusters).\n",
    "\n",
    "Then for rows that only contain zeros (meaning they still don't belong to a cluster) we calculate the mahalonobis distanse to all clusters and set them to the shortest on the fuzzy membership matrix.\n",
    "\n",
    "Next using GaussianMixture from sklearn.mixture on fuzzy matrix we assign each point to the cluster that has the most probability to be a part of and we store them on self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>3.4 Outlier Detection</u>\n",
    "\n",
    "In the previous computations, each point has been assigned to a projected cluster. However, the data might contain outliers that we need to detect. Here, we tried to use a multivariate outlier detection. For each cluster core and each assigned instance, we compute the inverse of the covariance matrix, as well as the cluster mean. More precisely, this information can be used to compute the mahalanobis distance for each point to the mean in the respective cluster core. This distance measure finds outliers depending on the distribution pattern of data points. In order to detect outliers, we need a threshold or critical value. The cutoff value is computed by the Chi-square distribution, where the degrees of freedom is equivalent to the number of features and a confidence level of alpha = 0.001. The confidence level specifies the probability of failing to identify a outlier is less than alpha. \n",
    "\n",
    "In the next cell, we draw a two-dimensional random dataset from a multivariate normal distribution and added some outliers. We tested our algorithm with the original alpha=0.001, as well as with a increased alpha of 0.1. The results below show for each alpha value the data points labeled from left to right according to the ground truth, own P3C implementation and ELKI P3C implementation, respectively. \n",
    "\n",
    "For alpha=0.001 our algorithm performs much better than the ELKI implementation and reaches a NMI score of 0.92. However, the three outliers are not detected. The two closer points are assigned to the second cluster, where the third point is assigned to a separate third cluster. \n",
    "\n",
    "Increasing alpha to 0.1 shows a different result. Again, our algorithm performs better than ELKI and the NMI further increased to 0.94. In addition, our outlier detection can identify one outlier and assigns a label of '-1'. \n",
    "\n",
    "This result shows that the concept of our outlier detection is working. However, alpha is not a parameter to tune. One strength of P3C is its robustness to its only parameter, the poisson threshold. In addition, we encountered some runtime errors in our outlier detection, most likely due to singularity issues while computing the inverse of the covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create synthetic test data outliers\n",
    "N = 2\n",
    "samples = 40\n",
    "cov = np.eye(N)\n",
    "data1 = np.random.multivariate_normal([0,0], cov, samples)\n",
    "outliers = np.array([[10,9],[42,0],[4,2]])\n",
    "labels1 = np.zeros(samples)\n",
    "labels_out = np.full((3), -1)\n",
    "\n",
    "data2 = np.random.multivariate_normal([-16,-18], cov, samples)\n",
    "data2 = np.append(data2, outliers, axis=0)\n",
    "syn_data = np.append(data1, data2, axis=0)\n",
    "labels2 = np.ones(samples)\n",
    "\n",
    "labels_= np.append(labels1,labels2, axis=0)\n",
    "syn_label = np.append(labels_, labels_out, axis=0)\n",
    "\n",
    "\n",
    "# Run Algorithm\n",
    "poisson = 1e-100 #the poisson threshold, the only adjustable parameter of P3C\n",
    "p3c = P3C(poisson)\n",
    "p3c.fit_predict(syn_data)\n",
    "syn_pred_P3C = p3c.labels_\n",
    "syn_pred_ELKI = run_elki_P3C (syn_data, poisson=poisson)\n",
    "\n",
    "# Compute NMI scores\n",
    "print('Outlier detection with alpha=0.001')\n",
    "print (\" NMI of own implementation:\", normalized_mutual_info_score (syn_pred_P3C, syn_label))\n",
    "print (\" NMI of ELKI implementation:\", normalized_mutual_info_score (syn_pred_ELKI, syn_label))\n",
    "\n",
    "# Visualize outlier results\n",
    "fig = plt.figure(figsize=(35, 10))\n",
    "ax0 = fig.add_subplot(131)\n",
    "ax0.scatter(syn_data[:,0], syn_data[:,1], c=syn_label, s=80)\n",
    "ax0.set_title(\"Ground Truth\", fontsize=30)\n",
    "ax0.axis('equal')\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.scatter(syn_data[:,0], syn_data[:,1], c=syn_pred_P3C, s=80)\n",
    "ax1.set_title(\"Own P3C Implementation\", fontsize=30)\n",
    "ax1.axis('equal')\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.scatter(syn_data[:,0], syn_data[:,1], c=syn_pred_ELKI, s=80)\n",
    "ax2.set_title(\"ELKI P3C Implementation\", fontsize=30)\n",
    "ax2.axis('equal')\n",
    "plt.show ()\n",
    "\n",
    "# Our labels\n",
    "print(syn_pred_P3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlier detection with alpha=0.1\n",
    "<img src='images/outlier_01.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>3.5 Relevant Attributes Detection</u>\n",
    "\n",
    "This part has not been implemented because it is more of a theoretical improvement for corner cases. It is also not implemented in ELKI, because it cannot be easily implemented in the same way as the rest of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Evaluation of the Algorithm</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now demonstrate the application of our P3C implementation on a synthetic 3D dataset with labels and compare it to the ELKI implementation. Three dimensions have been chosen for visualization purposes, but we will also demonstrate below, that it works with far more dimensions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data, syn_label = make_blobs(n_samples=500,\n",
    "                                 n_features=3,\n",
    "                                 centers=3,\n",
    "                                 cluster_std=.5,\n",
    "                                 random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "sub = fig.add_subplot(111, projection='3d')\n",
    "sub.scatter(syn_data[:,0], syn_data[:,1], syn_data[:,2], c= syn_label)\n",
    "sub.set_title(\"Synthetic Data\")\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson = 1e-100 #the poisson threshold, the only adjustable parameter of P3C\n",
    "p3c = P3C(poisson)\n",
    "p3c.fit_predict(syn_data)\n",
    "syn_pred_P3C = p3c.labels_\n",
    "syn_pred_ELKI = run_elki_P3C (syn_data, poisson=poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"NMI of own implementation:\", normalized_mutual_info_score (syn_pred_P3C, syn_label))\n",
    "print (\"NMI of ELKI implementation:\", normalized_mutual_info_score (syn_pred_ELKI, syn_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax0 = fig.add_subplot(131, projection='3d')\n",
    "ax0.scatter(syn_data[:,0], syn_data[:,1], syn_data[:,2], c=syn_label)\n",
    "ax0.set_title(\"Ground Truth\")\n",
    "ax1 = fig.add_subplot(132, projection='3d')\n",
    "ax1.scatter(syn_data[:,0], syn_data[:,1], syn_data[:,2], c=syn_pred_P3C)\n",
    "ax1.set_title(\"Own P3C Implementation\")\n",
    "ax2 = fig.add_subplot(133, projection='3d')\n",
    "ax2.scatter(syn_data[:,0], syn_data[:,1], syn_data[:,2], c=syn_pred_ELKI)\n",
    "ax2.set_title(\"ELKI P3C Implementation\")\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show, that we have found a superior implementation of P3C with respect to finding clusters. We suspect that our implementation of part 3.2 - the finding of cluster cores - is done in a more sophisticated way using an Apriori-like scheme. ELKI's Java implementation of P3C is, however, faster than ours.\n",
    "\n",
    "As we promised, we can show that those results hold also in higher dimensions - 30 in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data, syn_label = make_blobs(n_samples=500,\n",
    "                                 n_features=30,\n",
    "                                 centers=3,\n",
    "                                 cluster_std=.5,\n",
    "                                 random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson = 1e-100 #the poisson threshold, the only adjustable parameter of P3C\n",
    "p3c = P3C(poisson)\n",
    "p3c.fit_predict(syn_data)\n",
    "syn_pred_P3C = p3c.labels_\n",
    "syn_pred_ELKI = run_elki_P3C (syn_data, poisson=poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"NMI of own implementation:\", normalized_mutual_info_score (syn_pred_P3C, syn_label))\n",
    "print (\"NMI of ELKI implementation:\", normalized_mutual_info_score (syn_pred_ELKI, syn_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it can also be shown, that P3C is extremely unsensitive to the chosen Poisson threshold, as was also claimed in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data, syn_label = make_blobs(n_samples=100,\n",
    "                                 n_features=30,\n",
    "                                 centers=3,\n",
    "                                 cluster_std=.5,\n",
    "                                 random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poisson_list = [1e-10, 1e-20, 1e-30, 1e-40, 1e-50, 1e-60, 1e-70, 1e-80, 1e-90, 1e-100]\n",
    "syn_NMI_list = []\n",
    "for p in poisson_list:\n",
    "    p3c = P3C(p)\n",
    "    p3c.fit_predict(syn_data)\n",
    "    syn_NMI_list.append (normalized_mutual_info_score (p3c.labels_, syn_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_labels = ['1e-10', '1e-20', '1e-30', '1e-40', '1e-50', '1e-60', '1e-70', '1e-80', '1e-90', '1e-100']\n",
    "plt.bar (poisson_labels, syn_NMI_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will have a quick look at a real world dataset, namely the Boston housing dataset that was also used in the paper. It consists of 12 attributes of suburbs in Boston. It has no labels, so we cannot simply calculate an NMI. But, interestingly, we are able to reproduce what was found in the paper: The dataset cluster into 2 large clusters with a lot of smaller cluster and/or outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = np.genfromtxt ('housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson = 1e-100 #the poisson threshold, the only adjustable parameter of P3C\n",
    "p3c = P3C(poisson)\n",
    "p3c.fit_predict(house_data)\n",
    "syn_pred_ELKI = run_elki_P3C (house_data, poisson=poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max (syn_pred_ELKI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_p3c = []\n",
    "cluster_elki = []\n",
    "for i in range (len (p3c.cluster_centers_)):\n",
    "    count = 0\n",
    "    for entry in p3c.labels_:\n",
    "        if entry == i:\n",
    "            count += 1\n",
    "    cluster_p3c.append (count)\n",
    "for i in range (max (syn_pred_ELKI)):\n",
    "    count = 0\n",
    "    for entry in syn_pred_ELKI:\n",
    "        if entry == i:\n",
    "            count += 1\n",
    "    cluster_elki.append (count)\n",
    "print (\"Datapoints in clusters (own implementation):\\n\", cluster_p3c)\n",
    "print (\"Datapoints in clusters (ELKI implementation):\\n\", cluster_elki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that our implementation is able to better resamble the results from the paper than the ELKI implementation (for reasons already discussed in the evaluation of the synthetic data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HCV data set and seed data set have been chosen for this task.\n",
    "sources:\n",
    "    \n",
    "https://archive.ics.uci.edu/ml/datasets/HCV+data\n",
    "    \n",
    "https://archive.ics.uci.edu/ml/datasets/seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Data\n",
    "# HCV\n",
    "dataset_hcv = np.genfromtxt('hcvdat0.csv', delimiter=',')\n",
    "data_hcv = dataset_hcv[1:,2:]\n",
    "label_hcv = dataset_hcv[1:,1]\n",
    "\n",
    "# Seeds\n",
    "dataset_seeds = np.genfromtxt('seeds_dataset.txt')\n",
    "data_seeds = dataset_seeds[:,:dataset_seeds.shape[1]-1]\n",
    "label_seeds = dataset_seeds[:,dataset_seeds.shape[1]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of our P3C implementation with elki's P3C, dbscan and kmeans on HCV and Seeds dataset.\n",
    "\n",
    "HCV:\n",
    "\n",
    "    NMI of P3C on HCV dataset:\n",
    "    own implementation: 0.1262306702876908\n",
    "    ELKI implementation: 0.1886190630049598\n",
    "    ELKI implementation compared to own implementation: 0.3893625154244198\n",
    "    \n",
    "    NMI of dbscan on HCV dataset:\n",
    "    ELKI implementation: 0.0\n",
    "    \n",
    "    NMI of kmeans on HCV dataset:\n",
    "    ELKI implementation: 0.3157645024909989\n",
    "    ELKI implementation compared to own implementation P3C: 0.1209027291519607\n",
    "    \n",
    "Seeds:\n",
    "\n",
    "    NMI of P3C on Seeds dataset:\n",
    "    own implementation: 0.521508772947654\n",
    "    ELKI implementation: 0.0\n",
    "    ELKI implementation compared to own implementation: 0.0\n",
    "    \n",
    "    NMI of dbscan on Seeds dataset:\n",
    "    ELKI implementation: 0.14764823613022793\n",
    "    ELKI implementation compared to own implementation P3C: 0.0802847279078177\n",
    "    \n",
    "    NMI of kmeans on Seeds dataset:\n",
    "    ELKI implementation: 0.710063757776045\n",
    "    ELKI implementation compared to own implementation P3C: 0.4647854261028225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCV\n",
    "# P3C\n",
    "poisson = 1e-100\n",
    "p3c = P3C(poisson)\n",
    "p3c.fit_predict(data_hcv)\n",
    "hcv_pred_P3C = p3c.labels_\n",
    "hcv_pred_ELKI_P3C = run_elki_P3C(data_hcv, poisson=poisson)\n",
    "\n",
    "print(\"NMI of P3C on HCV dataset:\")\n",
    "print(\"own implementation:\", normalized_mutual_info_score (hcv_pred_P3C, label_hcv))\n",
    "print(\"ELKI implementation:\", normalized_mutual_info_score (hcv_pred_ELKI_P3C, label_hcv))\n",
    "print(\"ELKI implementation compared to own implementation:\", normalized_mutual_info_score (hcv_pred_ELKI_P3C, hcv_pred_P3C))\n",
    "print(\"\\n\")\n",
    "\n",
    "# SUBCLU\n",
    "# hcv_pred_ELKI_SUBCLU = run_elki_subclu(data_hcv, epsilon = 0.5, minpts = 3)\n",
    "\n",
    "# print (\"NMI of ELKI implementation of subclu on HCV dataset:\", normalized_mutual_info_score (hcv_pred_ELKI_SUBCLU, label_hcv))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# dbscan\n",
    "hcv_pred_ELKI_DBSCAN = run_elki_dbscan(data_hcv, epsilon = 0.3, minpts = 3)\n",
    "\n",
    "print(\"NMI of dbscan on HCV dataset:\")\n",
    "print (\"ELKI implementation:\", normalized_mutual_info_score (hcv_pred_ELKI_DBSCAN, label_hcv))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# kmeans\n",
    "hcv_pred_ELKI_KMEANS = run_elki_kmeans(data_hcv, k = 3)\n",
    "\n",
    "print(\"NMI of kmeans on HCV dataset:\")\n",
    "print (\"ELKI implementation:\", normalized_mutual_info_score (hcv_pred_ELKI_KMEANS, label_hcv))\n",
    "print(\"ELKI implementation compared to own implementation P3C:\", normalized_mutual_info_score (hcv_pred_ELKI_KMEANS, hcv_pred_P3C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI of P3C on Seeds dataset:\n",
      "own implementation: 0.522745529508714\n",
      "ELKI implementation: 0.0\n",
      "ELKI implementation compared to own implementation: 0.0\n",
      "\n",
      "\n",
      "NMI of ELKI implementation of PreDeCon on seeds dataset: 0.4082890254490723\n",
      "\n",
      "\n",
      "NMI of dbscan on Seeds dataset:\n",
      "ELKI implementation: 0.14764823613022793\n",
      "ELKI implementation compared to own implementation P3C: 0.08053447637569139\n",
      "\n",
      "\n",
      "NMI of kmeans on Seeds dataset:\n",
      "ELKI implementation: 0.6949250270680577\n",
      "ELKI implementation compared to own implementation P3C: 0.46455028935297216\n"
     ]
    }
   ],
   "source": [
    "# Seeds\n",
    "# P3C\n",
    "poisson = 1e-100\n",
    "p3c = P3C(poisson)\n",
    "p3c.fit_predict(data_seeds)\n",
    "seeds_pred_P3C = p3c.labels_\n",
    "seeds_pred_ELKI_P3C = run_elki_P3C(data_seeds, poisson=poisson)\n",
    "\n",
    "print(\"NMI of P3C on Seeds dataset:\")\n",
    "print (\"own implementation:\", normalized_mutual_info_score (seeds_pred_P3C, label_seeds))\n",
    "print (\"ELKI implementation:\", normalized_mutual_info_score (seeds_pred_ELKI_P3C, label_seeds))\n",
    "print (\"ELKI implementation compared to own implementation:\", normalized_mutual_info_score (seeds_pred_ELKI_P3C, seeds_pred_P3C))\n",
    "print(\"\\n\")\n",
    "\n",
    "#SUBCLU\n",
    "#seeds_pred_ELKI_SUBCLU = run_elki_subclu(syn_data, epsilon = 0.5, minpts = 3)\n",
    "\n",
    "#print (\"NMI of ELKI implementation of subclu on HCV dataset:\", normalized_mutual_info_score (seeds_pred_ELKI_SUBCLU, syn_label))\n",
    "#print(\"\\n\")\n",
    "\n",
    "#PreDeCon\n",
    "seeds_pred_ELKI_PreDeCon = run_elki_predecon(data_seeds, epsilon = 0.5, minpts = 3, delta=1e-5)\n",
    "\n",
    "print (\"NMI of ELKI implementation of PreDeCon on seeds dataset:\", normalized_mutual_info_score (seeds_pred_ELKI_PreDeCon, label_seeds))\n",
    "print(\"\\n\")\n",
    "\n",
    "# dbscan\n",
    "seeds_pred_ELKI_DBSCAN = run_elki_dbscan(data_seeds, epsilon = 0.3, minpts = 3)\n",
    "\n",
    "print(\"NMI of dbscan on Seeds dataset:\")\n",
    "print(\"ELKI implementation:\", normalized_mutual_info_score (seeds_pred_ELKI_DBSCAN, label_seeds))\n",
    "print(\"ELKI implementation compared to own implementation P3C:\", normalized_mutual_info_score (seeds_pred_ELKI_DBSCAN, seeds_pred_P3C))\n",
    "print(\"\\n\")\n",
    "\n",
    "# kmeans\n",
    "seeds_pred_ELKI_KMEANS = run_elki_kmeans(data_seeds, k = 3)\n",
    "print(\"NMI of kmeans on Seeds dataset:\")\n",
    "print (\"ELKI implementation:\", normalized_mutual_info_score (seeds_pred_ELKI_KMEANS, label_seeds))\n",
    "print(\"ELKI implementation compared to own implementation P3C:\", normalized_mutual_info_score (seeds_pred_ELKI_KMEANS, seeds_pred_P3C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work distribution within the group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manju Mariam Mathew left the group without contributing any working code parts or experiments. All the other four students contributed different yet similar important parts to the whole project. Workload was high due to the absence of one team member. Below, the main work packages of each of us are listed:\n",
    "\n",
    "* Robert Brunnsteiner: Class design P3C; P3C parts 3.1 and 3.3 implementation; ELKI scrip; Evaluation of own P3C implementation.\n",
    "* Mohammad Mahdi Fallah: P3C parts 3.1 and 3.3 implementation and report; Evaluation of P3C's strengths and comparison to other algorithms.\n",
    "* Akshey Kumar: P3C parts 3.2. and 3.4 implementation; Evaluation of outlier detection.\n",
    "* Jan-Jonas Schumacher: P3C parts 3.2. and 3.4 implementation and report; Evaluation of outlier detection.\n",
    "\n",
    "Those are just the main work packages. We always helped and assisted each other and discussed the results with each other!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
