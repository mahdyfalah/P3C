{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import poisson\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture # For Expectation maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = np.genfromtxt('scale-d-10d.csv', delimiter=' ')\n",
    "data = data[:,0:10]\n",
    "\n",
    "labels = np.genfromtxt('scale-d-10d.csv', delimiter=' ',dtype=\"|U5\")\n",
    "labels = labels[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler did not work properly!\n",
    "# https://datascience.stackexchange.com/questions/38004/minmaxscaler-returned-values-greater-than-one\n",
    "def rescale(data, new_min=0, new_max=1):\n",
    "    \"\"\"Rescale the data to be within the range [new_min, new_max]\"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rescale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies: numpy, scipy (for scipy.stats.chisquare)\n",
    "class P3C:\n",
    "    \n",
    "    #class variables\n",
    "    _alpha = 0.001 #alpha for chi-squared-test\n",
    "\n",
    "    #Poisson threshold is the only parameter for P3C \n",
    "    def __init__(self, poisson_threshold): \n",
    "        \n",
    "        #sklearn-like attributes\n",
    "        self.labels_ = None\n",
    "        self.cluster_centers_ = None #\"cluster cores\"\n",
    "        \n",
    "        #internally used variables\n",
    "        self._poisson_threshold = poisson_threshold\n",
    "        self._support_set = []\n",
    "        self._supports = []\n",
    "        self._approx_proj = []   #nested list: 1st level: attributes, 2nd level:\n",
    "                                 #different intervals, 3rd level: start and end of inteval.\n",
    "                                 #used as interface between part 3.1 and 3.2\n",
    "    \n",
    "    \n",
    "    # Methods for 3.1: Projections of true p-signatures (Mahdi and Robert)\n",
    "    \n",
    "    # Extra\n",
    "    def __compute_support(self, M):\n",
    "        '''Computes Supports of each bin\n",
    "        This function computes support set of each interval S and its support.\n",
    "        then assigns the values to self._support_set = [] and self._supports = []\n",
    "        SupportSet(S)= {x ∈ D | x.aj ∈ S }\n",
    "        Support(S) = |SupportSet(S)|\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        M : numpy.array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        '''\n",
    "        n = M.shape[0] # n = number of data objects\n",
    "        attribute_number = M.shape[1] # number of attributes \n",
    "        bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "\n",
    "\n",
    "        for i in range(attribute_number):\n",
    "            # support set of each interval S for attribiute i\n",
    "            supp_set = [[]for i in range(bin_number)]\n",
    "\n",
    "            interval_length = 1 / bin_number\n",
    "\n",
    "            # calculate in which bin should the point be placed based on attribute i\n",
    "            for j in range(n):\n",
    "                supp_set_index = math.floor(M[j,i]/interval_length)\n",
    "\n",
    "                if supp_set_index == len(supp_set):\n",
    "                    supp_set_index-=1 \n",
    "\n",
    "                supp_set[supp_set_index].append(M[j,:])\n",
    "\n",
    "            self._support_set.append(supp_set)\n",
    "            \n",
    "\n",
    "        self._supports = [[] for i in range(len(self._support_set))]\n",
    "        for i in range(len(self._support_set)):\n",
    "            for j in range(len(self._support_set[i])):\n",
    "                self._supports[i].append(len(self._support_set[i][j]))\n",
    "\n",
    "        return \n",
    "    \n",
    "    def __approximate_projection(self):\n",
    "        \n",
    "        bins = np.zeros((len(self._supports), len(self._supports[0])), dtype=int)\n",
    "        for attr_number in range(len(self._supports)): #loop over all attributes\n",
    "                \n",
    "            # part 1: create bin array            \n",
    "            supp = self._supports[attr_number].copy() #make a copy of the supports of the current attribute\n",
    "            while self.__uniformity_test(supp) == False: #if not uniform find highest element\n",
    "                max_index = supp.index(max(supp))\n",
    "                supp.pop(max_index) #remove highest element from list\n",
    "                \n",
    "                i = 1 \n",
    "                while i <= max_index: #loop to adjust max_index according to previousely deleted elements\n",
    "                    max_index += bins[attr_number, i]\n",
    "                    i += 1\n",
    "                bins[attr_number, max_index]  = 1 #mark highest bin\n",
    "            \n",
    "            #part 2: create _approx_proj list from bin array\n",
    "            interval_list = [] #2d list for current attribute\n",
    "            interval = [] #current interval\n",
    "            open_interval = False\n",
    "            \n",
    "            for i in range(len(bins[attr_number])):\n",
    "                if open_interval == False: #open new interval\n",
    "                    if bins[attr_number, i] == 1:\n",
    "                        interval.append(i)\n",
    "                        open_interval = True\n",
    "                if open_interval == True: #close current interval\n",
    "                    if bins[attr_number, i] == 0:\n",
    "                        interval.append(i)\n",
    "                        interval_list.append (interval)\n",
    "                        interval = []\n",
    "                        open_interval = False\n",
    "                    if (i == len(bins[attr_number])-1) and (bins[attr_number, i] == 1): #last bin marked 1\n",
    "                        interval.append (len(bins[attr_number]))\n",
    "                        interval_list.append (interval)                \n",
    "        \n",
    "            self._approx_proj.append (interval_list)\n",
    "        \n",
    "    def __uniformity_test(self, attr):\n",
    "        if chisquare(attr)[0] < self._alpha:\n",
    "            #print (\"uniform\")\n",
    "            return True\n",
    "        #print (\"non-uniform\")\n",
    "        return False  \n",
    "    \n",
    "    # Methods for 3.3: Cluster Cores (Akshey and Jonas)\n",
    "    \n",
    "    def __convert_approx_proj_to_dict(_approx_proj):\n",
    "        ''' Converts _approx_proj from 3.2 to a dictonary\n",
    "        This is necessary to compute cluster cores with apriori\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _approx_proj : three nested lists (# attribut/# interval/ start and end of interval)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _approx_proj_sig : list of dictonaries, each dictonary is a projection e.g {0: [0.1, 0.2]}\n",
    "\n",
    "        '''\n",
    "        _approx_proj_sig = []\n",
    "        for attribute, row in enumerate(_approx_proj):\n",
    "            for interval in row:\n",
    "                _approx_proj_sig.append({attribute:interval})\n",
    "                \n",
    "        return _approx_proj_sig\n",
    "    \n",
    "    \n",
    "     def __compute_support_sig(p_signature, dataset):\n",
    "        '''Computes support for p-signature\n",
    "        This function computes the support by removing data points\n",
    "        that do not lie in any of the intervals of the given p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        dataset : numpy.ndarray \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data.shape[0] : number of points in p-signature\n",
    "\n",
    "        '''\n",
    "\n",
    "        data = np.copy(dataset)\n",
    "        for attribute in p_signature:\n",
    "            interval = p_signature[attribute]\n",
    "            remove = []\n",
    "            for i, point in enumerate(data):\n",
    "                if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                    remove.append(i)\n",
    "            data = np.delete(data, remove, 0)\n",
    "\n",
    "        return data.shape[0]\n",
    "\n",
    "    \n",
    "    def __compute_exp_support(p_signature, interval, data):\n",
    "        ''' Computes expected support for a p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        interval : list with start and end value of interval\n",
    "\n",
    "        data : normalized np.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        support * width : \n",
    "\n",
    "        '''\n",
    "\n",
    "        support = __compute_support_sig(p_signature, data)\n",
    "        width = abs(interval[0] - interval[1])\n",
    "\n",
    "        return support*width\n",
    "    \n",
    "    \n",
    "    def __diff_interval(p_signature, pplus1_signature):\n",
    "        '''Helper function to compute difference in interval for two p-signatures.\n",
    "           Used for possion threshold\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interval : \n",
    "\n",
    "        '''\n",
    "\n",
    "        diff = list(set(pplus1_signature) - set(p_signature))\n",
    "        interval = pplus1_signature[diff[0]]\n",
    "        \n",
    "        return interval\n",
    "\n",
    "\n",
    "\n",
    "     def __check_core_condition(p_signature, pplus1_signature, dataset, threshold=1e-20):\n",
    "        ''' Checks if probability is smaller than possion threshold: \n",
    "        Possion(Supp(k+1 signature), ESupp(k+1 signature)) < possion_threshold\n",
    "        Returns True is poisson value is smaller than threshold\n",
    "\n",
    "        and\n",
    "\n",
    "        Checks if support is larger than expected support: \n",
    "        Supp(k+1 signature) > ESupp(k+1 siganature)\n",
    "        ESupp = Supp(S) * width(S')\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signatue : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        dataset : numpy.ndarray\n",
    "\n",
    "        threshold : poisson_threshold -> defined by user. default: 1e-20\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        true/false : \n",
    "\n",
    "        '''\n",
    "        \n",
    "        interval = __diff_interval(p_signature, pplus1_signature)\n",
    "        support = __compute_support_sig(pplus1_signature, dataset)\n",
    "        expected_support = __compute_exp_support(pplus1_signature, interval, dataset)\n",
    "        base_condition = support > expected_support\n",
    "        if base_condition:\n",
    "            poisson_value = poisson.pmf(support, expected_support) \n",
    "            if poisson_value < threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            \n",
    "    def __merge(dict1, dict2):\n",
    "        '''Helper function to merge to p-signature dictonaries \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dict1: p-signature dictonary\n",
    "\n",
    "        dict2: p-signature dictronary \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res : merged p-signature containing of dict1 and dict 2\n",
    "\n",
    "        '''\n",
    "        res = {**dict1, **dict2}\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def __a_is_subset_of_b(a,b):\n",
    "        '''Helper function that checks is dictionary a is a subset of dictionary b \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : dictionary\n",
    "\n",
    "        b : dictionary\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool : truth value\n",
    "\n",
    "        '''\n",
    "        return all((k in b and b[k]==v) for k,v in a.items())\n",
    "    \n",
    "    \n",
    "    def __apriori_cores(_approx_proj_sig, data):\n",
    "        ''' Computes cluster cores in apriori fashion. \n",
    "        The function computes maximal p-signatures that fulfill \n",
    "        two conditions. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _approx_proj_sig : list of dictonaries\n",
    "\n",
    "        data: dataset np.ndarray\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_centers_ : list of p-signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Loop through attributes and intervals (ignore same dimensions)\n",
    "        _cluster_cores = [_approx_proj_sig]\n",
    "\n",
    "        while _cluster_cores[-1] != []:\n",
    "            p_sig_list = []\n",
    "            for p_sig in _cluster_cores[-1]:\n",
    "                for one_sig in _approx_proj_sig:\n",
    "                  ### The second criterion is to avoid double counting\n",
    "                  ### In case this second criteria gives errors we could make a set of the p-signatures before checking the core condition\n",
    "                    if list(one_sig.keys())[0] not in p_sig.keys() and list(one_sig.keys())[0] > max(list(p_sig.keys())): \n",
    "                        pplus1_sig = __merge(p_sig, one_sig)\n",
    "                        ### Check core condition\n",
    "                        if __check_core_condition(p_sig, pplus1_sig, data):\n",
    "                            p_sig_list.append(pplus1_sig)\n",
    "            _cluster_cores.append(p_sig_list)\n",
    "        _cluster_cores.pop()\n",
    "\n",
    "        # Finds only the unique cluster cores since the above algorithm might be return the same core multiple times \n",
    "        cluster_centers_ = []\n",
    "        for p_sig_list in _cluster_cores:\n",
    "            for p_sig in p_sig_list:\n",
    "                if p_sig not in cluster_centers_:\n",
    "                    cluster_centers_.append(p_sig)\n",
    "\n",
    "        maximal_cluster_centers_ = cluster_centers_.copy()\n",
    "        # Check condition 2 for each signature (pruning to maximal cluster cores)\n",
    "        for cluster in reversed(cluster_centers_):\n",
    "            for sub_cluster in reversed(cluster_centers_):\n",
    "                if cluster != sub_cluster: \n",
    "                    if __a_is_subset_of_b(sub_cluster, cluster):\n",
    "                        if sub_cluster in maximal_cluster_centers_:\n",
    "                            maximal_cluster_centers_.remove(sub_cluster)\n",
    "\n",
    "\n",
    "        return maximal_cluster_centers_\n",
    "\n",
    "    def __compute_core_support(cores_p_signatures, data):\n",
    "        ''' Computes the support for each cluster core. This is necessary for 3.3:\n",
    "        computing the projected clusters \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cores_p_signatures : list of core signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cores_support_sets : list of supports for each cluster core\n",
    "\n",
    "        '''\n",
    "        cores_support_sets = []\n",
    "        for p_sig in cores_p_signatures:\n",
    "            cores_support_sets.append(__compute_support_sig(p_sig, data))\n",
    "        \n",
    "        return cores_support_sets\n",
    "    \n",
    "     # Methods for 3.3: Computing projected clusters (Manju)\n",
    "    def __fuzzy_membership_matrix(cluster_core_i,data): \n",
    "        '''\n",
    "        Refines the cluster cores into projected clusters\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_core_i:\n",
    "        \n",
    "        data:\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fuzzy_membership_matrix:\n",
    "        '''\n",
    "        fuzzy_membership_matrix=[]\n",
    "        for i in range(1,n):\n",
    "            for l in range(1,k):\n",
    "                if (i in data and l in support_set(cluster_core_i)):\n",
    "                    if(i not in support_set(cluster_core_i)): \n",
    "                        fuzzy_membership_matrix=0\n",
    "                       # unassigned_datapoints=cluster_core_i.append(i)\n",
    "                    elif (i in support_set(cluster_core_i)):\n",
    "                        fuzzy_membership_matrix=(1/support_set(cluster_core_i))[data]\n",
    "        \n",
    "        return fuzzy_membership_matrix   \n",
    "    \n",
    "                \n",
    "    def __probability_of_datapoint(fuzzy_membership_matrix,max_iterations):\n",
    "        '''\n",
    "          For each data point compute the probability of belonging to each projected cluster using Expectation \n",
    "          Maximization(EM)algorithm.\n",
    "          Parameters\n",
    "          ----------\n",
    "          fuzzy_membership_matrix:\n",
    "          \n",
    "          max_iterations:\n",
    "          \n",
    "          Returns\n",
    "          -------\n",
    "          probability_matrix:\n",
    "        \n",
    "        '''\n",
    "        # initialise EM with fuzzy_membership_matrix.cluster members have shorter mahalanobis distances to cluster\n",
    "        # means than non-cluster members\n",
    "        max_iterations=10\n",
    "        gaussian_mixture = GaussianMixture(n_components=2, covariance_type='full').fit(fuzzy_membership_matrix)\n",
    "        gaussian_mixture.means_\n",
    "        gaussian_mixture.fit()\n",
    "        label=gaussian_mixture.predict(fuzzy_membership_matrix)\n",
    "        \n",
    "        return gaussian_mixture\n",
    "        \n",
    "\n",
    "\n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit (self, X):\n",
    "        #all the method calls of 3.1 and 3.2 we have to implement go here...\n",
    "        self.__compute_support(X)\n",
    "        self.__approximate_projection()\n",
    "        \n",
    "        #self.cluster_center_ = ... #used as interface between part 3.2. and 3.3\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def predict (self, X):\n",
    "        pass #remove pass when implementing predict (X)\n",
    "        \n",
    "        #all the method calls of 3.3, 3.4 and 3.5 we have to implement go here...\n",
    "        \n",
    "        #self.labels_ = #final result of the algorithm.\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit_predict (self, X):\n",
    "        self.fit (X)\n",
    "        self.predict (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 14]], [[0, 14]], [[0, 1], [2, 14]], [[0, 1], [2, 14]], [[0, 14]], [[0, 14]], [[0, 14]], [[2, 14]], [[0, 1], [2, 14]], [[0, 1], [2, 14]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahdyfalah/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:6125: RuntimeWarning: invalid value encountered in true_divide\n",
      "  terms = (f_obs.astype(np.float64) - f_exp)**2 / f_exp\n",
      "/Users/mahdyfalah/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:6118: RuntimeWarning: Mean of empty slice.\n",
      "  f_exp = f_obs.mean(axis=axis, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "p3c = P3C (10)\n",
    "p3c.fit (data)\n",
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
