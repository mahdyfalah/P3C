{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import poisson\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture # For Expectation maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = np.genfromtxt('scale-d-10d.csv', delimiter=' ')\n",
    "data = data[:,0:10]\n",
    "\n",
    "labels = np.genfromtxt('scale-d-10d.csv', delimiter=' ',dtype=\"|U5\")\n",
    "labels = labels[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = np.genfromtxt ('housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Replaced by the method P3C.__normalize\\n# MinMaxScaler did not work properly!\\n# https://datascience.stackexchange.com/questions/38004/minmaxscaler-returned-values-greater-than-one\\ndef rescale(data, new_min=0, new_max=1):\\n    Rescale the data to be within the range [new_min, new_max]\\n    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Replaced by the method P3C.__normalize\n",
    "# MinMaxScaler did not work properly!\n",
    "# https://datascience.stackexchange.com/questions/38004/minmaxscaler-returned-values-greater-than-one\n",
    "def rescale(data, new_min=0, new_max=1):\n",
    "    Rescale the data to be within the range [new_min, new_max]\n",
    "    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies: numpy, scipy (for scipy.stats.chisquare), sklearn (for sklearn.preprocessing.MinMaxScaler)\n",
    "class P3C:\n",
    " \n",
    "    # General class functionality (Robert)\n",
    "    \n",
    "    #class variables\n",
    "    _alpha = 0.001 #alpha for chi-squared-test\n",
    "\n",
    "    #Poisson threshold is the only parameter for P3C \n",
    "    def __init__(self, poisson_threshold): \n",
    "        \n",
    "        #sklearn-like attributes\n",
    "        self.labels_ = [] \n",
    "        self.cluster_centers_ = None #\"cluster cores\"\n",
    "        \n",
    "        #internally used variables\n",
    "        self._X = None\n",
    "        self._poisson_threshold = poisson_threshold\n",
    "        self._support_set = []\n",
    "        self._supports = []\n",
    "        self._approx_proj = []\n",
    "        self._support_set_of_cores = []\n",
    "        self._membership_matrix = None #np.array: dimension: number of samples x number of clusters\n",
    "        \n",
    "    \n",
    "    def __convert_matrix_to_labels (self):\n",
    "        \"\"\"Converts membership matrix to labels\"\"\"\n",
    "        \n",
    "        for sample in self._membership_matrix:\n",
    "            for entry in range(sample.size):\n",
    "                if sample[entry] == 1:\n",
    "                    self.labels_.append(entry)\n",
    "    \n",
    "    # Methods for 3.1: Projections of true p-signatures (Mahdi and Robert)\n",
    "\n",
    "    def __normalize(self, X):\n",
    "        '''Normalizes the data featurewise\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        X : numpy.array of the input data \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array of the normalized data\n",
    "        \n",
    "        '''\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "    \n",
    "    def __uniformity_test(self, attr):\n",
    "        '''Uses the chi-squared test to determine if the attribute is uniformly disributed,\n",
    "        using the class variable self._alpha as a certrainty-threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        attr: input list to be tested for uniformity \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True, if input data is uniform\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if chisquare(attr)[0] < self._alpha:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def __compute_support(self):\n",
    "        '''Computes Supports of each bin\n",
    "        This function computes support set of each interval S and its support.\n",
    "        then assigns the values to self._support_set = [] and self._supports = []\n",
    "        SupportSet(S)= {x ∈ D | x.aj ∈ S }\n",
    "        Support(S) = |SupportSet(S)|\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self, \n",
    "\n",
    "        M : numpy.array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        '''\n",
    "        n = self._X.shape[0] # n = number of data objects\n",
    "        attribute_number = self._X.shape[1] # number of attributes \n",
    "        bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "\n",
    "\n",
    "        for i in range(attribute_number):\n",
    "            # support set of each interval S for attribiute i\n",
    "            supp_set = [[]for i in range(bin_number)]\n",
    "\n",
    "            interval_length = 1 / bin_number\n",
    "\n",
    "            # calculate in which bin should the point be placed based on attribute i\n",
    "            for j in range(n):\n",
    "                supp_set_index = math.floor(self._X[j,i]/interval_length)\n",
    "\n",
    "                if supp_set_index == len(supp_set):\n",
    "                    supp_set_index-=1 \n",
    "\n",
    "                supp_set[supp_set_index].append(self._X[j,:])\n",
    "\n",
    "            self._support_set.append(supp_set)\n",
    "            \n",
    "\n",
    "        self._supports = [[] for i in range(len(self._support_set))]\n",
    "        for i in range(len(self._support_set)):\n",
    "            for j in range(len(self._support_set[i])):\n",
    "                self._supports[i].append(len(self._support_set[i][j]))\n",
    "\n",
    "        return \n",
    "    \n",
    "    def __approximate_projection(self):\n",
    "        \n",
    "        '''Finds the bins that violate uniform distribution from the data stored in self._supports and\n",
    "        storest them in the numpy.array bins, with 1 for non-uniformity and 0 for uniformity. It then \n",
    "        assignes the intervals to self._approx_proj\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        '''\n",
    "        \n",
    "        bins = np.zeros((len(self._supports), len(self._supports[0])), dtype=int)\n",
    "        for attr_number in range(len(self._supports)): #loop over all attributes\n",
    "                            \n",
    "            supp = self._supports[attr_number].copy() #make a copy of the supports of the current attribute\n",
    "            while self.__uniformity_test(supp) == False: #if not uniform find highest element\n",
    "                max_index = supp.index(max(supp))\n",
    "                supp.pop(max_index) #remove highest element from list\n",
    "                \n",
    "                i = 1 \n",
    "                while i <= max_index: #loop to adjust max_index according to previousely deleted elements\n",
    "                    max_index += bins[attr_number, i]\n",
    "                    i += 1\n",
    "                bins[attr_number, max_index]  = 1 #mark highest bin\n",
    "        \n",
    " \n",
    "\n",
    "            interval_list = [] #2d list for current attribute\n",
    "            interval = [] #current interval\n",
    "            open_interval = False\n",
    "\n",
    "            for i in range(len(bins[attr_number])):\n",
    "                if open_interval == False: #open new interval\n",
    "                    if bins[attr_number, i] == 1:\n",
    "                        interval.append(i/len(bins[attr_number]))\n",
    "                        open_interval = True\n",
    "                if open_interval == True: #close current interval\n",
    "                    if bins[attr_number, i] == 0:\n",
    "                        interval.append(i/len(bins[attr_number]))\n",
    "                        interval_list.append (interval)\n",
    "                        interval = []\n",
    "                        open_interval = False\n",
    "                    if (i == len(bins[attr_number])-1) and (bins[attr_number, i] == 1): #last bin marked 1\n",
    "                        interval.append (len(bins[attr_number]))\n",
    "                        interval_list.append (interval)                \n",
    "\n",
    "            self._approx_proj.append (interval_list)\n",
    "            \n",
    "    \n",
    "    # Methods for 3.3: Cluster Cores (Akshey and Jonas)\n",
    "    \n",
    "    def __convert_approx_proj_to_dict(self):\n",
    "        ''' Converts _approx_proj from 3.2 to a dictonary\n",
    "        This is necessary to compute cluster cores with apriori\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self._approx_proj : three nested lists (# attribut/# interval/ start and end of interval)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _approx_proj_sig : list of dictonaries, each dictonary is a projection e.g {0: [0.1, 0.2]}\n",
    "\n",
    "        '''\n",
    "        _approx_proj_sig = []\n",
    "        for attribute, row in enumerate(self._approx_proj):\n",
    "            for interval in row:\n",
    "                _approx_proj_sig.append({attribute:interval})\n",
    "                \n",
    "        return _approx_proj_sig\n",
    "    \n",
    "    \n",
    "    def __compute_support_sig(self, p_signature):\n",
    "        '''Computes support for p-signature\n",
    "        This function computes the support by removing data points\n",
    "        that do not lie in any of the intervals of the given p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        dataset : numpy.ndarray \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data.shape[0] : number of points in p-signature\n",
    "\n",
    "        '''\n",
    "\n",
    "        data = np.copy(self._X)\n",
    "        for attribute in p_signature:\n",
    "            interval = p_signature[attribute]\n",
    "            remove = []\n",
    "            for i, point in enumerate(data):\n",
    "                if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                    remove.append(i)\n",
    "            data = np.delete(data, remove, 0)\n",
    "\n",
    "        return data.shape[0]\n",
    "\n",
    "    \n",
    "    def __compute_exp_support(self, p_signature, interval):\n",
    "        ''' Computes expected support for a p-signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        interval : list with start and end value of interval\n",
    "\n",
    "        data : normalized np.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        support * width : \n",
    "\n",
    "        '''\n",
    "\n",
    "        support = self.__compute_support_sig(p_signature)\n",
    "        width = abs(interval[0] - interval[1])\n",
    "\n",
    "        return support*width\n",
    "    \n",
    "    \n",
    "    def __diff_interval(self, p_signature, pplus1_signature):\n",
    "        '''Helper function to compute difference in interval for two p-signatures.\n",
    "           Used for possion threshold\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        interval : \n",
    "\n",
    "        '''\n",
    "\n",
    "        diff = list(set(pplus1_signature) - set(p_signature))\n",
    "        interval = pplus1_signature[diff[0]]\n",
    "        \n",
    "        return interval\n",
    "\n",
    "\n",
    "\n",
    "    def __check_core_condition(self, p_signature, pplus1_signature, threshold=1e-20):\n",
    "        ''' Checks if probability is smaller than possion threshold: \n",
    "        Possion(Supp(k+1 signature), ESupp(k+1 signature)) < possion_threshold\n",
    "        Returns True is poisson value is smaller than threshold\n",
    "\n",
    "        and\n",
    "\n",
    "        Checks if support is larger than expected support: \n",
    "        Supp(k+1 signature) > ESupp(k+1 siganature)\n",
    "        ESupp = Supp(S) * width(S')\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signatue : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "\n",
    "        dataset : numpy.ndarray\n",
    "\n",
    "        threshold : poisson_threshold -> defined by user. default: 1e-20\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        true/false : \n",
    "\n",
    "        '''\n",
    "        \n",
    "        interval = self.__diff_interval(p_signature, pplus1_signature)\n",
    "        support = self.__compute_support_sig(pplus1_signature)\n",
    "        expected_support = self.__compute_exp_support(pplus1_signature, interval)\n",
    "        base_condition = support > expected_support\n",
    "        if base_condition:\n",
    "            poisson_value = poisson.pmf(support, expected_support) \n",
    "            if poisson_value < threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            \n",
    "    def __merge(self, dict1, dict2):\n",
    "        '''Helper function to merge to p-signature dictonaries \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dict1: p-signature dictonary\n",
    "\n",
    "        dict2: p-signature dictronary \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res : merged p-signature containing of dict1 and dict 2\n",
    "\n",
    "        '''\n",
    "        res = {**dict1, **dict2}\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def __a_is_subset_of_b(self, a,b):\n",
    "        '''Helper function that checks is dictionary a is a subset of dictionary b \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : dictionary\n",
    "\n",
    "        b : dictionary\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool : truth value\n",
    "\n",
    "        '''\n",
    "        return all((k in b and b[k]==v) for k,v in a.items())\n",
    "    \n",
    "    \n",
    "    def __apriori_cores(self, _approx_proj_sig):\n",
    "        ''' Computes cluster cores in apriori fashion. \n",
    "        The function computes maximal p-signatures that fulfill \n",
    "        two conditions. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _approx_proj_sig : list of dictonaries\n",
    "\n",
    "        data: dataset np.ndarray\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_centers_ : list of p-signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Loop through attributes and intervals (ignore same dimensions)\n",
    "        _cluster_cores = [_approx_proj_sig]\n",
    "\n",
    "        while _cluster_cores[-1] != []:\n",
    "            p_sig_list = []\n",
    "            for p_sig in _cluster_cores[-1]:\n",
    "                for one_sig in _approx_proj_sig:\n",
    "                  ### The second criterion is to avoid double counting\n",
    "                  ### In case this second criteria gives errors we could make a set of the p-signatures before checking the core condition\n",
    "                    if list(one_sig.keys())[0] not in p_sig.keys() and list(one_sig.keys())[0] > max(list(p_sig.keys())): \n",
    "                        pplus1_sig = self.__merge(p_sig, one_sig)\n",
    "                        ### Check core condition\n",
    "                        if self.__check_core_condition(p_sig, pplus1_sig):\n",
    "                            p_sig_list.append(pplus1_sig)\n",
    "            _cluster_cores.append(p_sig_list)\n",
    "        _cluster_cores.pop()\n",
    "\n",
    "        # Finds only the unique cluster cores since the above algorithm might be return the same core multiple times \n",
    "        cluster_centers_ = []\n",
    "        for p_sig_list in _cluster_cores:\n",
    "            for p_sig in p_sig_list:\n",
    "                if p_sig not in cluster_centers_:\n",
    "                    cluster_centers_.append(p_sig)\n",
    "\n",
    "        maximal_cluster_centers_ = cluster_centers_.copy()\n",
    "        # Check condition 2 for each signature (pruning to maximal cluster cores)\n",
    "        for cluster in reversed(cluster_centers_):\n",
    "            for sub_cluster in reversed(cluster_centers_):\n",
    "                if cluster != sub_cluster: \n",
    "                    if self.__a_is_subset_of_b(sub_cluster, cluster):\n",
    "                        if sub_cluster in maximal_cluster_centers_:\n",
    "                            maximal_cluster_centers_.remove(sub_cluster)\n",
    "\n",
    "\n",
    "        self.cluster_centers_ = maximal_cluster_centers_\n",
    "\n",
    "    \"\"\"\n",
    "    def __compute_core_support(self, cores_p_signatures):\n",
    "        ''' Computes the support number for each cluster core.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cores_p_signatures : list of core signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cores_support_number : list of support numbers for each cluster core\n",
    "\n",
    "        '''\n",
    "        cores_support_number = []\n",
    "        for p_sig in cores_p_signatures:\n",
    "            cores_support_number.append(__compute_support_sig(p_sig))\n",
    "        \n",
    "        return cores_support_number\n",
    "    \"\"\"\n",
    "    \n",
    "    def __compute_core_set(self):\n",
    "        ''' Computes the support set for each cluster core. This is necessary for 3.3:\n",
    "        computing the projected clusters \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cores_p_signatures : list of core signatures e.g. [{0: [0.1, 0.2]}, {1: [0.5, 0.8], 2: [0.1, 0.4], 3: [0.5, 0.7]}]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cores_set : list of support sets for each cluster core\n",
    "\n",
    "        '''\n",
    "        \n",
    "        for p_signature in self.cluster_centers_:\n",
    "            dataset = np.copy(self._X)\n",
    "            for attribute in p_signature:\n",
    "                interval = p_signature[attribute]\n",
    "                remove = []\n",
    "                for i, point in enumerate(dataset):\n",
    "                    if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                        remove.append(i)\n",
    "                dataset = np.delete(dataset, remove, 0)\n",
    "            self._support_set_of_cores.append(dataset)\n",
    "    \n",
    "     # Methods for 3.3: Computing projected clusters (Manju)\n",
    "    def __fuzzy_membership_matrix(self): \n",
    "        '''\n",
    "        Refines the cluster cores into projected clusters\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_core_i:\n",
    "        \n",
    "        data:\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fuzzy_membership_matrix: \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        n = self._X.shape[0]\n",
    "        k = len(self.cluster_centers_)\n",
    "        \n",
    "        self._fuzzy_membership_matrix = np.array((n,k))\n",
    "        for i in range(0,n):\n",
    "            for l in range(0,k):\n",
    "                if (self._X[i] not in self._support_set_of_cores[l]):\n",
    "                    self._fuzzy_membership_matrix=0 # not returning the matrix values\n",
    "                    \n",
    "                elif (self._X[i] in self._support_set_of_cores[l]):\n",
    "                    self._fuzzy_membership_matrix=([1/self._support_set_of_cores[l]],(self._X[i]))\n",
    "        \n",
    "        return self._fuzzy_membership_matrix      \n",
    "   \n",
    " \n",
    "    \n",
    "                \n",
    "    def __probability_of_datapoint(self,max_iterations=10):\n",
    "        '''\n",
    "          For each data point compute the probability of belonging to each projected cluster using Expectation \n",
    "          Maximization(EM)algorithm.\n",
    "          Parameters\n",
    "          ----------\n",
    "          fuzzy_membership_matrix:\n",
    "          \n",
    "          max_iterations:\n",
    "          \n",
    "          Returns\n",
    "          -------\n",
    "          membership_matrix:\n",
    "        \n",
    "        '''\n",
    "        # initialise EM with fuzzy_membership_matrix.cluster members have shorter mahalanobis distances to cluster\n",
    "        # means than non-cluster members\n",
    "        gaussian_mixture = GaussianMixture(n_components=10, covariance_type='full').fit_predit(self._fuzzy_membership_matrix)\n",
    "        gaussian_mixture.means_\n",
    "        gaussian_mixture.covariances_\n",
    "        label=gaussian_mixture.fit_predict(self._fuzzy_membership_matrix)\n",
    "        membership_matrix=gaussian_mixture.append(data[i])\n",
    "        return membership_matrix\n",
    "        \n",
    "\n",
    "\n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit (self, X):\n",
    "        self._X = self.__normalize(X)\n",
    "        self.__compute_support()\n",
    "        self.__approximate_projection()\n",
    "        self.__apriori_cores(self.__convert_approx_proj_to_dict())\n",
    "        self.__compute_core_set()\n",
    "        self.__fuzzy_membership_matrix()\n",
    "        self.__convert_matrix_to_labels()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.cluster_center_ = ... #used as interface between part 3.2. and 3.3\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def predict (self, X):\n",
    "        pass #remove pass when implementing predict (X)\n",
    "        \n",
    "        #all the method calls of 3.3, 3.4 and 3.5 we have to implement go here...\n",
    "        \n",
    "        #self.labels_ = #final result of the algorithm.\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit_predict (self, X):\n",
    "        self.fit (X)\n",
    "        self.predict (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "print (house_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/stats/stats.py:5048: RuntimeWarning: invalid value encountered in true_divide\n",
      "  terms = (f_obs - f_exp)**2 / f_exp\n",
      "/usr/lib/python3/dist-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/usr/lib/python3/dist-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/usr/lib/python3/dist-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/usr/lib/python3/dist-packages/scipy/stats/stats.py:5037: RuntimeWarning: Mean of empty slice.\n",
      "  f_exp = np.atleast_1d(f_obs.mean(axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[449, 39, 10, 2, 2, 1, 1, 1, 1], [372, 47, 23, 13, 12, 4, 6, 19, 10], [75, 111, 64, 49, 18, 132, 30, 15, 12], [471, 0, 0, 0, 0, 0, 0, 0, 35], [99, 81, 104, 57, 56, 48, 37, 8, 16], [4, 4, 16, 101, 218, 103, 35, 15, 10], [15, 33, 43, 42, 39, 38, 48, 72, 176], [164, 117, 84, 54, 43, 28, 11, 4, 1], [82, 251, 41, 0, 0, 0, 0, 0, 132], [65, 107, 101, 67, 29, 0, 0, 0, 137], [16, 1, 61, 34, 62, 72, 59, 156, 45], [19, 9, 8, 2, 2, 8, 10, 28, 420], [86, 127, 100, 86, 51, 25, 17, 8, 6], [24, 70, 116, 164, 48, 36, 17, 9, 22]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-625881534fa2>:477: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self._fuzzy_membership_matrix=([1/self._support_set_of_cores[l]],(self._X[i]))\n"
     ]
    }
   ],
   "source": [
    "p3c = P3C (10)\n",
    "p3c.fit (house_data)\n",
    "print (p3c._supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0.0, 0.1111111111111111], 2: [0.0, 0.3333333333333333], 3: [0.0, 0.1111111111111111], 4: [0.0, 0.3333333333333333], 8: [0.0, 0.2222222222222222], 9: [0.0, 0.4444444444444444], 12: [0.0, 0.3333333333333333]}\n"
     ]
    }
   ],
   "source": [
    "print(p3c.cluster_centers_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "print(p3c.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.0, 0.1111111111111111]], [[0.0, 0.3333333333333333], [0.4444444444444444, 0.5555555555555556]], [[0.0, 0.3333333333333333], [0.5555555555555556, 0.6666666666666666]], [[0.0, 0.1111111111111111], [0.7777777777777778, 0.8888888888888888]], [[0.0, 0.3333333333333333]], [[0.2222222222222222, 9]], [[0.1111111111111111, 9]], [[0.0, 0.1111111111111111]], [[0.0, 0.2222222222222222], [0.8888888888888888, 9]], [[0.0, 0.4444444444444444], [0.8888888888888888, 9]], [[0.0, 0.1111111111111111], [0.2222222222222222, 9]], [[0.0, 0.1111111111111111], [0.2222222222222222, 0.3333333333333333], [0.5555555555555556, 0.6666666666666666], [0.7777777777777778, 9]], [[0.0, 0.3333333333333333]], [[0.0, 0.6666666666666666], [0.7777777777777778, 0.8888888888888888]]]\n"
     ]
    }
   ],
   "source": [
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([[           inf, 5.55555556e+00, 1.47459459e+01, ...,\n",
      "        1.00000000e+00, 1.11507692e+01, 2.36842105e+00],\n",
      "       [4.23867937e+03,            inf, 4.12708018e+00, ...,\n",
      "        1.00000000e+00, 4.89068826e+00, 2.71084337e+00],\n",
      "       [4.24272198e+03,            inf, 4.12708018e+00, ...,\n",
      "        1.01036916e+00, 1.57565217e+01, 1.51515152e+00],\n",
      "       ...,\n",
      "       [8.26089879e+03, 1.11111111e+00, 1.74871795e+01, ...,\n",
      "        1.03238403e+00, 1.30830325e+01, 1.79282869e+00],\n",
      "       [2.42490815e+03, 1.25000000e+00, 1.88137931e+01, ...,\n",
      "        1.03686467e+00, 5.73417722e+00, 3.40909091e+00],\n",
      "       [8.87303082e+02, 1.25000000e+00, 1.88137931e+01, ...,\n",
      "        1.05552007e+00, 9.43750000e+00, 2.88461538e+00]])], array([4.61841693e-04, 0.00000000e+00, 4.20454545e-01, 0.00000000e+00,\n",
      "       3.86831276e-01, 4.73079134e-01, 8.02265705e-01, 1.25071611e-01,\n",
      "       0.00000000e+00, 1.64122137e-01, 8.93617021e-01, 1.00000000e+00,\n",
      "       1.69701987e-01, 1.53333333e-01]))\n"
     ]
    }
   ],
   "source": [
    "print(p3c._fuzzy_membership_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
