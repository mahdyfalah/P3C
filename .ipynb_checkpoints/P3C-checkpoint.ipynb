{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import poisson\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture # For Expectation maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = np.genfromtxt('scale-d-10d.csv', delimiter=' ')\n",
    "data = data[:,0:9]\n",
    "\n",
    "labels = np.genfromtxt('scale-d-10d.csv', delimiter=' ',dtype=\"|U5\")\n",
    "labels = labels[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies: numpy, scipy (for scipy.stats.chisquare)\n",
    "class P3C:\n",
    "    \n",
    "    #class variables\n",
    "    _alpha = 0.001 #alpha for chi-squared-test\n",
    "\n",
    "    #Poisson threshold is the only parameter for P3C \n",
    "    def __init__(self, poisson_threshold): \n",
    "        \n",
    "        #sklearn-like attributes\n",
    "        self.labels_ = None\n",
    "        self.cluster_centers_ = None #\"cluster cores\"\n",
    "        \n",
    "        #internally used variables\n",
    "        self._poisson_threshold = poisson_threshold\n",
    "        self._support_set = []\n",
    "        self._supports = []\n",
    "        self._approx_proj = []   #nested list: 1st level: attributes, 2nd level:\n",
    "                                 #different intervals, 3rd level: start and end of inteval.\n",
    "                                 #used as interface between part 3.1 and 3.2\n",
    "    \n",
    "    \n",
    "    # Methods for 3.1: Projections of true p-signatures (Mahdi and Robert)\n",
    "    \n",
    "    # Compute Support set of all attributes\n",
    "    # SupportSet = {x ∈ D | x.aj ∈ S }\n",
    "    def __compute_support(self, M):\n",
    "        n = M.shape[0] # n = number of data objects\n",
    "        attribute_number = M.shape[1] # number of attributes \n",
    "        bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "\n",
    "        for i in range(attribute_number):\n",
    "            supp_set = [[]for i in range(bin_number)] # a set containing supports of an attr\n",
    "                                                      # in different bins\n",
    "\n",
    "            # needs to get normalized \n",
    "            aj_max = np.max(M[:,i]) \n",
    "            aj_min = np.min(M[:,i])\n",
    "\n",
    "            interval_length = (aj_max - aj_min ) / bin_number\n",
    "\n",
    "            bins = np.zeros([bin_number])\n",
    "\n",
    "            for k in range(bin_number):\n",
    "                bins[k] = aj_min + (k+1)*interval_length\n",
    "\n",
    "            for j in range(n):\n",
    "                supp_set_index = 0\n",
    "                for k in range(len(bins)):\n",
    "                    if M[j,i] > bins[k]:\n",
    "                        supp_set_index += 1\n",
    "\n",
    "                # I believe some kind of rounding error happens here\n",
    "                if supp_set_index < len(supp_set):\n",
    "                    supp_set[supp_set_index].append(M[j])\n",
    "                else:\n",
    "                    supp_set[len(supp_set) - 1].append(M[j])\n",
    "\n",
    "\n",
    "            supp = []\n",
    "            for supp_pts in supp_set:\n",
    "                supp.append (len(supp_pts))\n",
    "            self._supports.append (supp)\n",
    "            self._support_set.append(supp_set)\n",
    "            \n",
    "            \n",
    "        # Uncomment the part below for projections\n",
    "#         for i in range (len(self._supports)):\n",
    "#             for j in range(len(self._supports[i])):\n",
    "#                 print(len(self._supports[i][j])) \n",
    "                \n",
    "        return\n",
    "    \n",
    "    def __approximate_projection(self):\n",
    "        \n",
    "        bins = np.zeros((len(self._supports), len(self._supports[0])), dtype=int)\n",
    "        for attr_number in range(len(self._supports)): #loop over all attributes\n",
    "                \n",
    "            # part 1: create bin array            \n",
    "            supp = self._supports[attr_number].copy() #make a copy of the supports of the current attribute\n",
    "            while self.__uniformity_test(supp) == False: #if not uniform find highest element\n",
    "                max_index = supp.index(max(supp))\n",
    "                supp.pop(max_index) #remove highest element from list\n",
    "                \n",
    "                i = 1 \n",
    "                while i <= max_index: #loop to adjust max_index according to previousely deleted elements\n",
    "                    max_index += bins[attr_number, i]\n",
    "                    i += 1\n",
    "                bins[attr_number, max_index]  = 1 #mark highest bin\n",
    "            \n",
    "            #part 2: create _approx_proj list from bin array\n",
    "            interval_list = [] #2d list for current attribute\n",
    "            interval = [] #current interval\n",
    "            open_interval = False\n",
    "            \n",
    "            for i in range(len(bins[attr_number])):\n",
    "                if open_interval == False: #open new interval\n",
    "                    if bins[attr_number, i] == 1:\n",
    "                        interval.append(i)\n",
    "                        open_interval = True\n",
    "                if open_interval == True: #close current interval\n",
    "                    if bins[attr_number, i] == 0:\n",
    "                        interval.append(i)\n",
    "                        interval_list.append (interval)\n",
    "                        interval = []\n",
    "                        open_interval = False\n",
    "                    if (i == len(bins[attr_number])-1) and (bins[attr_number, i] == 1): #last bin marked 1\n",
    "                        interval.append (len(bins[attr_number]))\n",
    "                        interval_list.append (interval)                \n",
    "        \n",
    "            self._approx_proj.append (interval_list)\n",
    "        \n",
    "    def __uniformity_test(self, attr):\n",
    "        if chisquare(attr)[0] < self._alpha:\n",
    "            #print (\"uniform\")\n",
    "            return True\n",
    "        #print (\"non-uniform\")\n",
    "        return False  \n",
    "    \n",
    "    # Methods for 3.3: Cluster Cores (Akshey and Jonas)\n",
    "    \n",
    "    def __compute_support_sig(p_signature, dataset):\n",
    "        '''Computes support for p-signature\n",
    "        This function computes the support by removing data points\n",
    "        that do not lie in any of the intervals of the given p-signature\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "        \n",
    "        dataset : numpy.ndarray \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data.shape[0] : number of points in p-signature\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        data = np.copy(dataset)\n",
    "        for attribute in p_signature:\n",
    "            interval = p_signature[attribute]\n",
    "            remove = []\n",
    "            for i, point in enumerate(data):\n",
    "                if  interval[0] > point[attribute] or point[attribute] > interval[1]:\n",
    "                    remove.append(i)\n",
    "            data = np.delete(data, remove, 0)\n",
    "        return data.shape[0]\n",
    "    \n",
    "    \n",
    "    def __compute_exp_support(p_signature, interval, data):\n",
    "        ''' Computes expected support for a p-signature\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "            \n",
    "        interval : list with start and end value of interval\n",
    "        \n",
    "        data : normalized np.ndarray\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        support * width : \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        support = __compute_support_sig(p_signature, data)\n",
    "        width = abs(interval[0] - interval[1])\n",
    "        return support*width\n",
    "    \n",
    "    \n",
    "    def __diff_interval(p_signature, pplus1_signature):\n",
    "        '''Helper function to compute difference in interval for two p-signatures.\n",
    "           Used for possion threshold\n",
    "           \n",
    "        Parameters\n",
    "        ---------- \n",
    "        p_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "         \n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "           \n",
    "        Returns\n",
    "        -------\n",
    "        interval : \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        diff = list(set(pplus1_signature) - set(p_signature))\n",
    "        interval = pplus1_signature[diff[0]]\n",
    "        return interval\n",
    "\n",
    " \n",
    "    def __check_core_condition(p_signature, pplus1_signature, dataset, threshold=1e-20):\n",
    "        ''' Checks if probability is smaller than possion threshold: \n",
    "        Possion(Supp(k+1 signature), ESupp(k+1 signature)) < possion_threshold\n",
    "        Returns True is poisson value is smaller than threshold\n",
    "\n",
    "        and\n",
    "\n",
    "        Checks if support is larger than expected support: \n",
    "        Supp(k+1 signature) > ESupp(k+1 siganature)\n",
    "        ESupp = Supp(S) * width(S')\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        p-signatue : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "        \n",
    "        pplus1_signature : dictronary e.g. {0:[0,0.1], 3:[0.1,0.2]} -> Intervals for attributes 0 and 3\n",
    "        \n",
    "        dataset : numpy.ndarray\n",
    "        \n",
    "        threshold : poisson_threshold -> defined by user. default: 1e-20\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        true/false : \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        interval = diff_interval(p_signature, pplus1_signature)\n",
    "        support = __compute_support_sig(pplus1_signature, dataset)\n",
    "        expected_support = __compute_exp_support(pplus1_signature, interval, dataset)\n",
    "        base_condition = support > expected_support\n",
    "        if base_condition:\n",
    "            possion_value = poisson.pmf(support, expected_support) \n",
    "            if poisson_value < threshold:\n",
    "                return True\n",
    "    \n",
    "    \n",
    "    def __apriori_cores(approx_proj, supports):\n",
    "        ''' Computes cluster cores in apriori fashion. \n",
    "        The function computes maximal p-signatures that fulfill \n",
    "        two conditions. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        approx_proj :\n",
    "\n",
    "        supports : \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_p_signatures : \n",
    "\n",
    "        '''\n",
    "        \n",
    "          # Loop through attributes and intervals (ignore same dimensions)\n",
    "\n",
    "            # Compute k+1 signatures from valid k signatures\n",
    "\n",
    "            # Check condition 1 for each signature (check_supp_expected_supp())\n",
    "\n",
    "            # Check condition 2 for each signature\n",
    "\n",
    "            # Prune away infrequent k+1 signatures\n",
    "\n",
    "          # Select maximal p-signatures\n",
    "\n",
    "        pass\n",
    "    \n",
    "     # Methods for 3.3: Computing projected clusters (Manju)\n",
    "    def __fuzzy_membership_matrix(cluster_core_i,data): \n",
    "        '''\n",
    "        Refines the cluster cores into projected clusters\n",
    "        Parameters\n",
    "        ----------\n",
    "        cluster_core_i:\n",
    "        \n",
    "        data:\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fuzzy_membership_matrix:\n",
    "        '''\n",
    "        fuzzy_membership_matrix=[]\n",
    "        for i in range(1,n):\n",
    "            for l in range(1,k):\n",
    "                if (i in data and l in support_set(cluster_core_i)):\n",
    "                    if(i not in support_set(cluster_core_i)): \n",
    "                        fuzzy_membership_matrix=0\n",
    "                       # unassigned_datapoints=cluster_core_i.append(i)\n",
    "                    elif (i in support_set(cluster_core_i)):\n",
    "                        fuzzy_membership_matrix=(1/support_set(cluster_core_i))[data]\n",
    "        \n",
    "        return fuzzy_membership_matrix   \n",
    "    \n",
    "                \n",
    "    def __probability_of_datapoint(fuzzy_membership_matrix,max_iterations):\n",
    "        '''\n",
    "          For each data point compute the probability of belonging to each projected cluster using Expectation \n",
    "          Maximization(EM)algorithm.\n",
    "          Parameters\n",
    "          ----------\n",
    "          fuzzy_membership_matrix:\n",
    "          \n",
    "          max_iterations:\n",
    "          \n",
    "          Returns\n",
    "          -------\n",
    "          probability_matrix:\n",
    "        \n",
    "        '''\n",
    "        # initialise EM with fuzzy_membership_matrix.cluster members have shorter mahalanobis distances to cluster\n",
    "        # means than non-cluster members\n",
    "        max_iterations=10\n",
    "        gaussian_mixture = GaussianMixture(n_components=2, covariance_type='full').fit(fuzzy_membership_matrix)\n",
    "        gaussian_mixture.means_\n",
    "        gaussian_mixture.fit()\n",
    "        label=gaussian_mixture.predict(fuzzy_membership_matrix)\n",
    "        \n",
    "        return gaussian_mixture\n",
    "        \n",
    "\n",
    "\n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit (self, X):\n",
    "        #all the method calls of 3.1 and 3.2 we have to implement go here...\n",
    "        self.__compute_support(X)\n",
    "        self.__approximate_projection()\n",
    "        \n",
    "        #self.cluster_center_ = ... #used as interface between part 3.2. and 3.3\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def predict (self, X):\n",
    "        pass #remove pass when implementing predict (X)\n",
    "        \n",
    "        #all the method calls of 3.3, 3.4 and 3.5 we have to implement go here...\n",
    "        \n",
    "        #self.labels_ = #final result of the algorithm.\n",
    "        \n",
    "    #data X is numpy.ndarray with samples x features (no label!)  \n",
    "    def fit_predict (self, X):\n",
    "        self.fit (X)\n",
    "        self.predict (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 14]], [[1, 14]], [[0, 1], [2, 14]], [[0, 13]], [[1, 14]], [[1, 13]], [[1, 14]], [[2, 14]], [[1, 14]]]\n"
     ]
    }
   ],
   "source": [
    "p3c = P3C (10)\n",
    "p3c.fit (data)\n",
    "print (p3c._approx_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1\n",
    "def proj_true_p_signature(M):\n",
    "    \n",
    "    # For every bin in every attribute, its support is computed\n",
    "    supp_sets = compute_support(M)\n",
    "    \n",
    "\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Support set of all attributes\n",
    "# SupportSet = {x ∈ D | x.aj ∈ S }\n",
    "def compute_support(M):\n",
    "    n = M.shape[0] # n = number of data objects\n",
    "    attribute_number = M.shape[1] # number of attributes \n",
    "    bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "    list_of_supports = [] # list of all supports\n",
    "\n",
    "    for i in range(attribute_number):\n",
    "        supp_set = [[]for i in range(bin_number)]\n",
    "\n",
    "        # needs to get normalized \n",
    "        aj_max = np.max(M[:,i])\n",
    "        aj_min = np.min(M[:,i])\n",
    "\n",
    "        interval_length = (aj_max - aj_min ) / bin_number\n",
    "\n",
    "        bins = np.zeros([bin_number])\n",
    "\n",
    "        for k in range(bin_number):\n",
    "            bins[k] = aj_min + (k+1)*interval_length\n",
    "\n",
    "        for j in range(n):\n",
    "            supp_set_index = 0\n",
    "            for k in range(len(bins)):\n",
    "                if M[j,i] > bins[k]:\n",
    "                    supp_set_index += 1\n",
    "\n",
    "            # I believe some kind of rounding error happens here\n",
    "            if supp_set_index < len(supp_set):\n",
    "                supp_set[supp_set_index].append(M[j])\n",
    "            else:\n",
    "                supp_set[len(supp_set) - 1].append(M[j])\n",
    "\n",
    "\n",
    "        list_of_supports.append(supp_set)\n",
    "\n",
    "    \n",
    "    for i in range (len(list_of_supports)):\n",
    "        for j in range(len(list_of_supports[i])):\n",
    "            print(len(list_of_supports[i][j])) \n",
    "\n",
    "    return list_of_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65264191 -0.00371625  0.01521223 ...  0.21708319  0.78426444\n",
      "   0.27407853]\n",
      " [ 1.01930291  0.35519293  0.47796839 ...  0.60008908  0.35612478\n",
      "   0.24724175]\n",
      " [ 0.5940366  -0.33190342 -0.10154468 ...  0.68686449  0.53546747\n",
      "   1.27822433]\n",
      " ...\n",
      " [ 0.84569877  0.35925799  0.64490759 ...  0.87088832  0.63797899\n",
      "   0.87146842]\n",
      " [ 0.27499317  0.49355767  0.20943054 ...  0.93080848  0.72043327\n",
      "   1.42526599]\n",
      " [ 0.39640662  0.57944655  0.980204   ...  0.0876678   0.15534795\n",
      "   0.67756318]]\n",
      "(10000, 9)\n",
      "['C1' 'C1' 'C1' ... 'C100' 'C100' 'C100']\n",
      "(10000,)\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.shape)\n",
    "\n",
    "print(labels)\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "print(int(1 + math.log(data.shape[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "26\n",
      "76\n",
      "202\n",
      "1510\n",
      "3126\n",
      "2942\n",
      "1775\n",
      "211\n",
      "68\n",
      "34\n",
      "14\n",
      "4\n",
      "4\n",
      "11\n",
      "33\n",
      "79\n",
      "247\n",
      "1128\n",
      "2837\n",
      "3063\n",
      "2108\n",
      "319\n",
      "120\n",
      "36\n",
      "10\n",
      "5\n",
      "1\n",
      "0\n",
      "7\n",
      "44\n",
      "122\n",
      "330\n",
      "2254\n",
      "3315\n",
      "3113\n",
      "546\n",
      "180\n",
      "52\n",
      "26\n",
      "10\n",
      "7\n",
      "10\n",
      "44\n",
      "96\n",
      "269\n",
      "2312\n",
      "2852\n",
      "2721\n",
      "1329\n",
      "232\n",
      "91\n",
      "26\n",
      "9\n",
      "2\n",
      "3\n",
      "7\n",
      "25\n",
      "87\n",
      "191\n",
      "817\n",
      "2962\n",
      "3155\n",
      "2219\n",
      "334\n",
      "126\n",
      "49\n",
      "19\n",
      "6\n",
      "3\n",
      "11\n",
      "32\n",
      "116\n",
      "316\n",
      "1923\n",
      "3127\n",
      "3049\n",
      "1079\n",
      "231\n",
      "82\n",
      "24\n",
      "4\n",
      "3\n",
      "3\n",
      "11\n",
      "29\n",
      "91\n",
      "218\n",
      "911\n",
      "2931\n",
      "3086\n",
      "2189\n",
      "344\n",
      "133\n",
      "41\n",
      "8\n",
      "5\n",
      "1\n",
      "1\n",
      "10\n",
      "18\n",
      "83\n",
      "257\n",
      "2402\n",
      "3591\n",
      "3080\n",
      "406\n",
      "121\n",
      "25\n",
      "3\n",
      "2\n",
      "2\n",
      "8\n",
      "32\n",
      "135\n",
      "315\n",
      "2101\n",
      "3103\n",
      "2850\n",
      "1087\n",
      "226\n",
      "91\n",
      "39\n",
      "6\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M = np.array([[1, 2], [4, 5], [3, 3], [6, 7]])\n",
    "print(compute_support(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([1, 2])], [array([4, 5]), array([3, 3])], [array([6, 7])]]\n",
      "[[array([1, 2]), array([3, 3])], [array([4, 5])], [array([6, 7])]]\n"
     ]
    }
   ],
   "source": [
    "# Extra\n",
    "M = np.array([[1, 2], [4, 5], [3, 3], [6, 7]])\n",
    "n = M.shape[0] # n = number of data objects\n",
    "attribute_number = M.shape[1] # number of attributes \n",
    "bin_number = int(1 + math.log(n,2)) # number of bins\n",
    "list_of_supports = [] # list of all supports\n",
    "\n",
    "for i in range(attribute_number):\n",
    "    supp_set = [[]for i in range(3)]\n",
    "    \n",
    "    aj_max = np.max(M[:,i])\n",
    "    aj_min = np.min(M[:,i])\n",
    "    interval_length = (aj_max - aj_min ) / bin_number\n",
    "    bins = np.zeros([bin_number])\n",
    "    for k in range(bin_number):\n",
    "        bins[k] = aj_min + (k+1)*interval_length\n",
    "        \n",
    "    for j in range(n):\n",
    "        supp_set_index = 0\n",
    "        for k in range(len(bins)):\n",
    "            if M[j,i] > bins[k]:\n",
    "                supp_set_index += 1\n",
    "        if supp_set_index < len(supp_set):\n",
    "            supp_set[supp_set_index].append(M[j])\n",
    "        else:\n",
    "            supp_set[len(supp_set) - 1].append(M[j])\n",
    "    \n",
    "    print(supp_set)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([4, 5])], [], [array([6, 7])]]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.array([[1, 2], [4, 5], [3, 3], [6, 7]])\n",
    "\n",
    "supp_set = [[]for i in range(3)]\n",
    "supp_set[0].append(M[1])\n",
    "supp_set[2].append(M[3])\n",
    "\n",
    "supp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.333333333333334"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2*interval_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2b66fd261ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
